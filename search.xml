<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Lambda&Stream.md]]></title>
    <url>%2F2019%2F05%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2FLamda%E7%A7%AF%E7%B4%AF%2F</url>
    <content type="text"><![CDATA[Lambda&amp;Stream积累 LambdaLambda主要是一个类语法长糖,尽量为java引入函数编程等实现,细节后续再补充 Streamjava8提拱的新特性之一就有stream.stream主要是针对集合的处理类.提供了一系列集合处理方式.配合使用lambda写出简介优美的代码 Stream的使用通过如123456List&lt;Integer&gt; list = new ArrayList&lt;&gt;();list.stream();//即可以开启串行流;list.parallelStream().filter(a -&gt; &#123; return a &gt; 20; &#125;);//开启并行流 串行流即内部单线程顺序执行,并行则是启用多线程执行.后者并不一定效率就比前者高.因为并行执行启用分配线程资源时同样要消耗时间和资源,在一定量级下,前者的执行效率一度要高过后者. 我这里对三种对集合的处理形式的比较,可以简单参考一下 stream 串行流 parallelStream 并行流 常规循环式 12345678910111213141516171819202122232425262728293031List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 100000; i++) &#123; list.add(getRandomNum()); &#125; DateUtil.setBegin(); list.stream().filter(a -&gt; &#123; return a &gt; 20; &#125;); DateUtil.setStop(); System.out.println("串行耗时"+DateUtil.calCostTime()); DateUtil.setBegin(); list.parallelStream().filter(a -&gt; &#123; return a &gt; 20; &#125;); DateUtil.setStop(); System.out.println("并行耗时"+DateUtil.calCostTime()); int count = 0; DateUtil.setBegin(); for (int l : list) &#123; if (l &gt; 20) &#123; count++; &#125; &#125; DateUtil.setStop(); System.out.println("循环耗时"+DateUtil.calCostTime()); 经由相当量次的测试后,我觉得如果要对集合中的数据进行遍历操作,根据量级的不同,建议低量级还是采用普通循环,量级特别大,可考虑用并行流.书写方便,又不是大批量数据处理操作可以直接采用串行流 Stream的操作分类 Intermediate Terminal Short-circuiting]]></content>
      <tags>
        <tag>日常总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop HA搭建]]></title>
    <url>%2F2019%2F05%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fhadoop%2FhadoopHA%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[HAlkajdlqj艺术硕士asdafdasdfasd艺术硕士asasd]]></content>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka学习]]></title>
    <url>%2F2019%2F05%2F06%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fkafka%2Fkafka%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[[ x ] Consumer Group里只会被某一个Consumer消费 ,Kafka还允许不同Consumer Group同时消费同一条消息，这一特性可以为消息的多元化处理提供支持。 kafka 发送模式通过producer.type设置,可以设置producer的发送模式,具体参数据有producer.type=false即同步(默认就是同步),设置为true为异步,即以batch形式像broker发送信息.(这里的batch可以设置)还有一种oneway.即通过对ack的设置即可实现,ack=0时,即为oneway,只管发,不管是否接收成功.-1则是全部副本接收成功才算成功. kakfa消费模式 at last one at most one exactly one]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F22%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fsparkstreaming%E7%AE%97%E5%AD%90%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[mapreduce组件总结]]></title>
    <url>%2F2019%2F04%2F19%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fspark%E5%AD%A6%E4%B9%A03%2F</url>
    <content type="text"><![CDATA[spark-core,spark-streaming再深造 spark go on初始规划 spark-corespark-streaming]]></content>
      <tags>
        <tag>bigdata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据分享]]></title>
    <url>%2F2019%2F04%2F15%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[开头语工欲善其事，必先利其器本次分享,是我在公司的第一次分享,我考虑后,将本次分享的主要内容分为了三大部块.先是针对相关基础组件分类介绍.再介绍下通过对这些组件进行组织配搭的大数据基础环境架构.再结合我的一些经历,为大家介绍下相关的应用与产品落地. 技术栈简介 数据采集 数据存储 数据治理(清洗&amp;处理) 数据应用 产品落地 我又根据不同组件的特性将他们分 采集类 存储类 计算处理类 传输类 管理类 其它类 下面开始具体介绍 采集类数据源: 日志 业务数据 公网数据(爬虫) 文本数据 出行数据(gps,手机定位等) sqoop flume crawler datax kettle elk Flume(水槽) 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单,可拓展 Sqoop是一个在结构化数据和Hadoop之间进行批量数据迁移的工具，结构化数据可以是Mysql、Oracle等RDBMS。Sqoop底层用MapReduce程序实现抽取、转换、加载，MapReduce天生的特性保证了并行化和高容错率，而且相比Kettle等传统ETL工具，任务跑在Hadoop集群上，减少了ETL服务器资源的使用情况。在特定场景下，抽取过程会有很大的性能提升。 crawler , jsoup ,httpclient, nutch 等. elk 集中式日志系统 ELK 协议栈详解 存储类 hdfs hbase hive mongdb redis RDBMS hdfs* 分布式文件存储系统 * 提供了高可靠性、高扩展性和高吞吐率的数据存储服务 * hdfs典型结构：物理结构+逻辑结构 * 文件线性切割成Block：偏移量（offset） * Block分散存储在集群节点中 * 单一文件Block大小一致，文件与文件可以不一致 * Block可以设置副本数，副本分散在不同的节点中 * 副本数不要超过节点数量 * 文件上传可以设置Block大小和副本数 * 已上传的文件Block副本数可以调整，大小不变 * 只支持一次写入多次读取，同一时刻只有一个写入者 * 只能追加，不能修改 hbaseHBase是一个构建在HDFS上的分布式列存储系统；HBase是基于Google BigTable模型开发的，典型的key/value系统；HBase是Apache Hadoop生态系统中的重要一员，主要用于海量结构化数据存储； 大：一个表可以有数十亿行，上百万列；无模式：每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一张表中不同的行可以有截然不同的列；面向列：面向列（族）的存储和权限控制，列（族）独立检索；稀疏：空（null）列并不占用存储空间，表可以设计的非常稀疏；数据多版本：每个单元中的数据可以有多个版本，默认情况下版本号自动分配，是单元格插入时的时间戳；数据类型单一：Hbase中的数据都是字符串，没有类型 openTSDB基于Hbase的分布式的，可伸缩的时间序列数据库。主要用途，就是做监控系统；譬如收集大规模集群（包括网络设备、操作系统、应用程序）的监控数据并进行存储，查询。 solr &amp; Phoenix二级索引 hiveHive 是一个基于 Hadoop 文件系统之上的数据仓库架构。它可以将结构化的数据文件映射为一张数据库表，并提供简单的 sql 查询功能。还可以将 sql 语句转换为 MapReduce 任务运行。底部计算引擎还可以用用Tez, spark等. ImpalaImpala是Cloudera公司推出，提供对HDFS、Hbase数据的高性能、低延迟的交互式SQL查询功能。 基于Hive使用内存计算，兼顾数据仓库、具有实时、批处理、多并发等优点 对内存依赖大,稳定性不如hive 相比hive数据仓库,impala针对的量级相关少些,但会有效率的提升.但一般来讲,数据仓库一类需求对时间上的要要求一般不会太高,所以常规方式一般就符合大多数需求. 计算处理类 mapreduce mapreduce on oozie ,on tez spark flink mapreduceMapreduce是一个计算框架，既然是做计算的框架，那么表现形式就是有个输入（input），mapreduce操作这个输（input），通过本身定义好的计算模型，得到一个输出（output），这个输出就是我们所需要的结果。我们要学习的就是这个计算模型的运行规则。在运行一个mapreduce计算任务时候，任务过程被分为两个阶段：map阶段和reduce阶段，每个阶段都是用键值对（key/value）作为输入（input）和输出（output）。而程序员要做的就是定义好这两个阶段的函数：map函数和reduce函数。 分布式计算；移动计算而不移动数据。 spark相比一二代计算引擎,在兼并了一二代的特色之外,还引放了流计算这一能力,还丰富了计算函数.其中比较有代表性的主要就是spark&amp;storm.也就是说这代计算引擎兼具无边界数据与有边界数据同样的处理能力.同时还具有DAG特性.这里主要介绍spark spark主要组成有以下 spark-core spark-streaming spark-sql spark-mlib spark-graphX。 spark-core是一个提供内存计算的框架,其他的四大框架都是基于spark core上进行计算的,所以没有spark core,其他的框架是浮云.spark-core的主要内容就是对RDD的操作RDD的创建 -&gt;RDD的转换 -&gt;RDD的缓存 -&gt;RDD的行动 -&gt;RDD的输出 spark-streaming中使用离散化流（discretized stream）作为抽象的表示，叫做DStream。它是随时间推移而收集数据的序列，每个时间段收集到的数据在DStream内部以一个RDD的形式存在。DStream支持从kafka，flume,hdfs,s3等获取输入。DStream也支持两种操作，即转化操作和输出操作 spark-sqlSpark SQL 提供了查询结构化数据及计算结果等信息的接口.查询结果以 Datasets and DataFrames 形式返回 … flink/blink略 传输类kafkaKafka是分布式发布-订阅消息系统,一个分布式的，可划分的，冗余备份的持久性的日志服务。它主要用于处理活跃的流式数据。日常中常与spark-streaming结合实用,为其提供无边界数据 管理类 Hue cloudera-managerhue与cm 都是由cloudera提供,后面cloudera将hue开源给了apache.如果基础集群环境是采用的是开源自主搭建,可考虑引入hue.另一些大数据服务公司,有集成打包自己的一些大数据产品,如cdh等.但这些服务收费,涉及到成本问题.所以如何选用,需要相关斟酌. 其它类 zookeeper ,yarn等zookeeper在集中基础环境中主要作为配置分享中心,与kafka,hbase等组件集成.yarn则作为资源管理组件,可以与mapreduce ,spark等集成 各类组件架构以上,已经大致介绍了各类工具,基本了解了相应的特性和使用场景,而根据它们的特性,进行合理的配备,架构,从而实现一个功能全面,稳定的大数据环境. 于我个人经历与平时了解来讲,一般的架构主要如下另: 总得来说,各类组件供选型一般来讲都不是单一的.所以,我们的大数据环境各部份组件都是插销式可插拔的.所以不同公司可能不一而同,具体看自身需求和实际情况.比如上图中的storm流式计算模块,就可以替换成spark-streaming等. 通过对上图的架构的拆解,再组合,可能还会有以下组织架构. 数据仓库可以理解为上图中间部份.作为一个数据集市的存在,算作数据中心的一部份. ODS：是数据仓库第一层数据，直接从原始数据过来的，经过简单地处理，比喻：字段体重的数据为175cm等数据。 DW*：这个是数据仓库的第二层数据，DWD和DWS很多情况下是并列存在的，这一层储存经过处理后的标准数据，比喻订单、用户、页面点击流量等数据。 ADS：这个是数据仓库的最后一层数据，为应用层数据，直接可以给业务人员使用。 星型模型 星型模型中有两个重要的概念：事实表和维度表。事实表：一些主键ID的集合，没有存放任何实际的内容维度表：存放详细的数据信息，有唯一的主键ID。如上面的关键词表、用户表等等。 数据中心:概念相对更大一些,可能即作为具体平台产品集合,也可能是一个团队行政划分.总得来说,是如 大数据基础平台 数据仓库 DMP平台 相关应用平台如推荐系统,报表系统,可视化平台等. 数据中台:这个是由阿里于15年率先提出.主导思想是大中台,小前台.这块暂无特别明确的解释说法,但现在也有不少公司效仿.我个人从它的主导思想”大中台,小前台”的理解是,这个可能是体量更大,壁垒更少的一个数据集成体.比如阿里系的旗下公司,数据流都会归集到中台,同时阿里系下的公司也能获得不仅自己公司数据中心归集的数据反馈,还能获得阿里中台整合后流出的反馈数据. 应用落地公共服务 交通出行 智慧城市 … 产品应用 用户画像 征信模型 推荐系统 精确营销 前沿科学(无人驾驶,人工智能,AR等) 结语以上,就是我今天分享的主要内容.今天的主题是”器”,但对这些工具的讲解浅尝辄止,在实际的开发实战中涉及的情况是更为复杂,需要掌握的内容更多,深度也更深.我这里主要是想抛砖引玉,为大家提供一点自己的理解,若能有所帮助,不胜荣幸. 另外,工具始终是工具,菜刀再利也要厨子好,才能做好菜.所以如何利用这些工具,与我们的业务结合,实现我们想要的价值,这是我一直在探索的,也愿与各位同仁一同前行. 附上图中涉及到的技术栈]]></content>
      <tags>
        <tag>日常总结</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F11%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fkafka%26spark%2F</url>
    <content type="text"><![CDATA[kafka to spark]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F11%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fspark%E5%AD%A6%E4%B9%A02%2F</url>
    <content type="text"><![CDATA[spark学习2spark 运行的四种模式 本地模式如1./bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[1] ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100 standlone模式client./bin/spark-submit –class org.apache.spark.examples.SparkPi –master spark://spark001:7077 –executor-memory 1G –total-executor-cores 1 ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100 cluster./bin/spark-submit –class org.apache.spark.examples.SparkPi –master spark://spark001:7077 –deploy-mode cluster –supervise –executor-memory 1G –total-executor-cores 1 ./lib/spark-examples-1.3.1-hadoop2.7.0.jar 100 Yarn模式client模式123client模式：结果xshell可见：./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --executor-memory 1G --num-executors 1 ./lib/spark-examples-1.3.1-hadoop2.7.0.jar 100 cluster模式./bin/spark-submit –class org.apache.spark.examples.SparkPi –master yarn-cluster –executor-memory 1G –num-executors 1 ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100 spark sql]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F10%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fflume%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[flume记录from kafka 123456789101112131415161718192021222324252627282930313233343536373839a1.sources = source1a1.sources.source1.type = org.apache.flume.source.kafka.KafkaSourcea1.sources.source1.channels = c1a1.sources.source1.batchSize = 5000a1.sources.source1.batchDurationMillis = 2000a1.sources.source1.zookeeperConnect = localhost:2181#a1.sources.source1.kafka.brokerList = localhost:9092a1.sources.source1.kafka.bootstrap.servers = localhost:9092a1.sources.source1.topic = flumetesta1.sources.source1.kafka.consumer.group.id = custom.g.ida1.channels = c1a1.channels.c1.type = memorya1.channels.c1.capacity = 10000a1.channels.c1.transactionCapacity = 10000a1.channels.c1.byteCapacityBufferPercentage = 20a1.channels.c1.byteCapacity = 800000a1.sinks = k1a1.sinks.k1.type = file_rolla1.sinks.k1.channel = c1a1.sinks.k1.sink.directory = /home/hadoop/testfile/flume 这里也有版本匹配的问题.经过多番尝试,这里的组合版本是flume1.6+kafka_2.11-2.2.0.tgz其它版本可能会有request header 问题.另外还遇到了指定topic 和 zookeeper的问题. 执行语句:flume-ng agent -n a1 -c conf -f kafka.properties -Dflume.root.logger=INFO,console flume 采集到kafka123456789101112131415161718192021222324252627agent.sources=r1agent.sinks=k1agent.channels=c1agent.sources.r1.type=execagent.sources.r1.command=tail /root/tomcat/logs/catalina.outagent.sources.r1.restart=trueagent.sources.r1.batchSize=1000agent.sources.r1.batchTimeout=3000agent.sources.r1.channels=c1agent.channels.c1.type=memoryagent.channels.c1.capacity=102400agent.channels.c1.transactionCapacity=1000agent.channels.c1.byteCapacity=134217728agent.channels.c1.byteCapacityBufferPercentage=80agent.sinks.k1.channel=c1agent.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSinkagent.sinks.k1.kafka.topic=sparkstreamingagent.sinks.k1.kafka.zookeeperConnect=47.102.199.215:2181#agent.sinks.k1.kafka.bootstrap.servers=47.102.199.215:9092agent.sinks.k1.kafka.brokerList =47.102.199.215:9092agent.sinks.k1.serializer.class=kafka.serializer.StringEncoderagent.sinks.k1.flumeBatchSize=1000agent.sinks.k1.useFlumeEventFormat=true]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F10%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fspark%E9%9B%86%E7%BE%A4%E6%93%8D%E4%BD%9C%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[#操作记录 ./spark-submit –master yarn-cluster –class practice.spark.task.SparkTestDemo bigdata-jar-with-dependencies.jar 10]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F10%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2FYarn%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Yarn配置细节##内存,核数设置 1234567891011121314151617181920212223242526272829303132 &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;3072&lt;/value&gt;&lt;/property&gt;&lt;!--该配置用于配置任务请求时的资源. --&gt;&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;/name&gt; &lt;value&gt;-Xmx3276m&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[spark实战]]></title>
    <url>%2F2019%2F03%2F21%2Fspark%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[spark实战spark-core]]></content>
      <tags>
        <tag>bigdata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mapreduce组件总结]]></title>
    <url>%2F2019%2F03%2F19%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fmapreduce%E7%BB%84%E4%BB%B6%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[mapreduce组件总结 相关组件大致有 Inputformat Inputsplit ReadRecorder mapper Combiner Partioner Reduce GroupComparator Reduce shuffle 1shuffle 被称为mapreduce的核心,一个真正让奇迹发生的地方.但它到底是什么呢?简练的讲,它就是 map out 到 reduce in 这段过程中对数据的处理过程. shuffle过程中主要发生的操作有,Partion,Sort,spill,merge,copy,sort,merge.(还有可能有combine操作) 具体流程是map out后,Collector 对out后的数据进行处理. 数据将会写入到内存缓冲区,该内存缓冲区的数据达到80%后,会开启一个溢写线程,在磁盘本地创建一个文件.如果reduce设置了多个分区,写入buffer区的数据,会被打上一个分区标记.通过sortAndSpill()方法进行指对数据按分区号,key排序.最后溢出的文件是分区的,按key有序的文件.若buffer区中的20%一直未被填满,buffer写入进程不会断.但若达到100%,Buffer写入进程则会阻塞.并在buffer区中的数据全部spill完后才会再开启. (buffer区的内存默认是100M),spill过程中,若设置过combiner.则会对数据先进行combiner逻辑处理,再将处理后的数据写出 spill完成后则会对本地的spill后的文件进行Merge.即把多个spill后的文件进行合并,并排序.最后会行成一个有序文件 当1个Map Task 完成后,reduce 就会开启copy进程(默认是5个线程).这个过程中会通过http请求去各taskTracker(nodemanager),拉取相应的spill&amp;merge后的文件.当copy完成后,则又会对数据进行merge.这个过程中同样有个类似map shuffle 中的buffer 溢写的阶段. 这个过程同样会触发combiner组件.这里的merge数据源有三种 memory to memory memory to disk disk to disk默认1是不开启的. copy phase 完成后,是reduceTask 中的 sort phase即对merge 中的文件继续进行sort and group . 当sort phase 完成.则开启reduce phase .到此shuffle正式完成. ##二次排序 mapreduce 常见的辅助排序 partitioner key的比较Comparator 分组函数Grouping Comparator joinmap join ,semi join ,reduce join ##]]></content>
      <tags>
        <tag>bigdata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[准备小结]]></title>
    <url>%2F2019%2F03%2F14%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2F%E5%87%86%E5%A4%87%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[准备小结 hdfs存储机制是怎样的?client端发送写文件请求，namenode检查文件是否存在，如果已存在，直接返回错误信息，否则，发送给client一些可用namenode节点client将文件分块，并行存储到不同节点上datanode上，发送完成后，client同时发送信息给namenode和datanodenamenode收到的client信息后，发送确信信息给datanodedatanode同时收到namenode和datanode的确认信息后，提交写操作。 hadoop中combiner的作用是什么?当map生成的数据过大时，带宽就成了瓶颈，怎样精简压缩传给Reduce的数据，又不影响最终的结果呢。有一种方法就是使用Combiner，Combiner号称本地的Reduce，Reduce最终的输入，是Combiner的输出。 你们数据库怎么导入hive 的,有没有出现问题在导入hive的时候，如果数据库中有blob或者text字段，会报错，解决方案在sqoop笔记中。在将数据由Oracle数据库导入到Hive时，发现带有clob字段的表的数据会错乱，出现一些字段全为NULL的空行。由于在项目中CLOB字段没有实际的分析用途，因此考虑将CLOB字段去掉。 hdfs-site.xml的3个主要属性?dfs.name.dir决定的是元数据存储的路径以及DFS的存储方式(磁盘或是远端)dfs.data.dir决定的是数据存储的路径fs.checkpoint.dir用于第二Namenode 下列哪项通常是集群的最主要瓶颈磁盘 IO答案：C 磁盘首先集群的目的是为了节省成本，用廉价的 pc 机，取代小型机及大型机。小型机和大型机有什么特点？1.cpu 处理能力强2.内存够大，所以集群的瓶颈不可能是 a 和 d3.如果是互联网有瓶颈，可以让集群搭建内网。每次写入数据都要通过网络（集群是内网），然后还要写入 3 份数据，所以 IO 就会打折扣。 关于 SecondaryNameNode 哪项是正确的？它的目的是帮助 NameNode 合并编辑日志，减少 NameNode 启动时间 mapreduce的原理?MapReduce采用”分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个分节点共同完成，然后通过整合各个节点的中间结果，得到最终结果。简单地说，MapReduce就是”任务的分解与结果的汇总”。在Hadoop中，用于执行MapReduce任务的机器角色有两个：一个是JobTracker；另一个是TaskTracker，JobTracker是用于调度工作的，TaskTracker是用于执行工作的。一个Hadoop集群中只有一台JobTracker。在分布式计算中，MapReduce框架负责处理了并行编程中分布式存储、工作调度、负载均衡、容错均衡、容错处理以及网络通信等复杂问题，把处理过程高度抽象为两个函数：map和reduce，map负责把任务分解成多个任务，reduce负责把分解后多任务处理的结果汇总起来。需要注意的是，用MapReduce来处理的数据集（或任务）必须具备这样的特点：待处理的数据集可以分解成许多小的数据集，而且每一个小数据集都可以完全并行地进行处理。 HDFS存储的机制?写流程：client链接namenode存数据namenode记录一条数据位置信息（元数据），告诉client存哪。client用hdfs的api将数据块（默认是64M）存储到datanode上。datanode将数据水平备份。并且备份完将反馈client。client通知namenode存储块完毕。namenode将元数据同步到内存中。另一块循环上面的过程。 读流程举一个简单的例子说明mapreduce是怎么来运行的 ?MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。 Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。Mapper任务的执行过程详解 每个Mapper任务是一个Java进程，它会读取HDFS中的文件，解析成很多的键值对，经过我们覆盖的map方法处理后，转换为很多的键值对再输出。整个Mapper任务的处理过程又可以分为以下六个阶段： 第一阶段是把输入文件按照一定的标准分片(InputSplit)，每个输入片的大小是固定的。默认情况下，输入片(InputSplit)的大小与数据块(Block)的大小是相同的。如果数据块(Block)的大小是默认值128MB，输入文件有两个，一个是32MB，一个是 172MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由 一个Mapper进程处理。这里的三个输入片，会有三个Mapper进程处理。 第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键”是每一 行的起始位置(单位是字节)，“值”是本行的文本内容。 第三阶段是调用Mapper类中的map方法。第二阶段中解析出来的每一个键值对，调用一次map方法。如果有1000个键值对，就会 调用1000次map方法。每一次调用map方法会输出零个或者多个键值对。 第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份(如北京、上海、 山东等)，那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是Reducer 任务运行的数量。默认只有一个Reducer任务。第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值 对&lt;2,2&gt;、&lt;1,3&gt;、&lt;2,1&gt;，键和值分别是整数。那么排序后的结果是&lt;1,3&gt;、&lt;2,1&gt;、&lt;2,2&gt;。如果有第六阶段，那么进入 第六阶段 如果没有，直接输出到本地的Linux文件中。 第六阶段是对数据进行归约处理，也就是reduce处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。 归约后的数据输出到本地的linxu文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。 Reducer任务的执行过程详解每个Reducer任务是一个java进程。Reducer任务接收Mapper任务的输出，归约处理后写入到HDFS中，可以分为三个阶段：第一阶段是Reducer任务会主动从Mapper任务复制其输出的键值对。Mapper任务可能会有很多，因此Reducer会复制多个Mapper的输出。第二阶段是把复制到Reducer本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。第三阶段是对排序后的键值对调用reduce方法。键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对。最后把这些输出的键值对写入到HDFS文件中。在整个MapReduce程序的开发过程中，我们最大的工作量是覆盖map函数和覆盖reduce函数。 了解hashMap 和hashTable吗介绍下，他们有什么区别。为什么重写equals还要重写hashcode因为equals比较的是内容是一致.但hashcode 说一下map的分类和常见的情况 hashmap,hashtable,treemap,LinkedHashMap 根据键得到值，因此不允许键重复(重复了覆盖了),但允许值重复Hashmap是一个最常用的Map 它根据键的HashCode值存储数据,根据键可以直接获取它的值，具有很快的访问速度，遍历时，取得数据的顺序是完全随机的 最多只允许一条记录的键为Null;允许多条记录的值为 Null; HashMap不支持线程的同步，即任一时刻可以有多个线程同时写HashMap;可能会导致数据的不一致。 如果需要同步，可以用 Collections的synchronizedMap方法使HashMap具有同步的能力，或者使用ConcurrentHashMapHashtableHashtable与 HashMap类似,它继承自Dictionary类,不同的是:它不允许记录的键或者值为空; 它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了 Hashtable在写入时会比较慢LinkedHashMap是 HashMap 的一个子类，保存了记录的插入顺序，在用 Iterator 遍历 LinkedHashMap 时，先得到的记录肯定是先插入的.也可以在构造时用带参数，按照应用次数排序。在遍历的时候会比 HashMap 慢，不过有种情况例外，当 HashMap 容量很大，实际数据较少时，遍历起来可能会比 LinkedHashMap 慢，因为 LinkedHashMap 的遍历速度只和实际数据有关，和容量无关，而 HashMap 的遍历速度和他的容量有关TreeMap实现 SortMap 接口,能够把它保存的记录根据键排序, 默认是按键值的升序排序，也可以指定排序的比较器，当用 Iterator 遍历 TreeMap 时，得到的记录是排过序的 HashMap，链表法存储，entry[]数组，线程不安全，可能死锁 concurrentHashMap，segment数组，每个segent下维护一组entry[]数组，每个segment是一把锁，线程安全 LinkedHashMap Object若不重写hashCode()的话，hashCode()如何计算出来的？hashcode采用的是 spark1. spark的有几种部署模式，每种模式特点？本地模式本地模式分三类 local：只启动一个executor local[k]: 启动k个executor local[*]：启动跟cpu数目相同的 executor cluster模式cluster模式肯定就是运行很多机器上了，但是它又分为以下三种模式，区别在于谁去管理资源调度。（说白了，就好像后勤管家，哪里需要资源，后勤管家要负责调度这些资源） standalone模式分布式部署集群，自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础 Spark on yarn模式分布式部署集群，资源和任务监控交给yarn管理粗粒度资源分配方式，包含cluster和client运行模式cluster 适合生产，driver运行在集群子节点，具有容错功能client 适合调试，dirver运行在客户端 2. Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？Spark core是其它组件的基础，spark的内核主要包含：有向循环图、RDD、Lingage、Cache、broadcast等 SparkStreaming是一个对实时数据流进行高通量、容错处理的流式处理系统将流式计算分解成一系列短小的批处理作业 Spark sql：能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询 MLBase是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低MLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime。 GraphX是Spark中用于图和图并行计算 spark有哪些组件master：管理集群和节点，不参与计算。worker：计算节点，进程本身不参与计算，和master汇报。Driver：运行程序的main方法，创建spark context对象。spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。client：用户提交程序的入口。 https://blog.csdn.net/yirenboy/article/details/47441465]]></content>
      <tags>
        <tag>小结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka]]></title>
    <url>%2F2019%2F03%2F11%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fkafka%2F</url>
    <content type="text"><![CDATA[kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic sparkstreamingkafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic flumetest kafka-console-producer.sh –broker-list localhost:9092 –topic flumetest :创建生产者 kafka-console-consumer.sh –bootstrap-server namenode:9092 –topic sparkstreaming Kafka相关小结kafka 相关指令kafka-server-start.sh config/server.properties &amp; 启动kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic topic_name :创建topickafka-console-producer.sh –broker-list localhost:9092 –topic topic_name :创建生产者 kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic topic_name :创建消费者 kafka-console-producer.sh –broker-list namenode:9092 –topic sparkstreaming kafka java apikafka 虽然搭建较为简单,但想要对针它编程体验还是有些问题.初步使用下来明显感觉对版本的强约束性.以我线上版本 为例,我java项目对应的版本则是1234567891011&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka_2.10&lt;/artifactId&gt; &lt;version&gt;0.8.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.8.2.1&lt;/version&gt; &lt;/dependency&gt; 以上版本搭配经由我亲测通过]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive总结]]></title>
    <url>%2F2018%2F12%2F24%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fhive%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Hive相关点小结 启动指令 hive == hive –service cli不需要启动server，使用本地的metastore，可以直接做一些简单的数据操作和测试。 启动hiveserver2hive –service hiveserver2 beeline工具测试使用jdbc方式连接beeline -u jdbc:hive2://localhost:10000 1.managed table管理表。删除表时，数据也删除了 2.external table外部表。删除表时，数据不删 建表:CREATE TABLE IF NOT EXISTS t2(id int,name string,age int)COMMENT ‘xx’ //注释ROW FORMAT DELIMITED //行分隔符FIELDS TERMINATED BY ‘,’ //字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改STORED AS TEXTFILE ; 外部表: CREATE TABLE IF NOT EXISTS t2(id int,name string,age int) COMMENT ‘xx’ ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ STORED AS TEXTFILE ; 分区表，桶表分区表Hive中有分区表的概念。我们可以看到分区表具有重要的性能，而且分区表还可以将数据以一种符合逻辑的方式进行组织，比如分层存储。Hive的分区表，是把数据放在满足条件的分区目录下CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) //按照年月进行分区 ROW FORMAT DELIMITED //行分隔符 FIELDS TERMINATED BY ‘,’ ; //字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改load data local inpath ‘/home/zpx/customers.txt’ into table t3 partition 分桶表这样做，在查找数据的时候就可以跨越多个桶，直接查找复合条件的数据了。速度快，时间成本低。Hive中的桶表默认使用的机制也是hash。CREATE TABLE t4(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS //创建3个通桶表，按照字段id进行分桶 ROW FORMAT DELIMITED //行分隔符 FIELDS TERMINATED BY &apos;,&apos; ; load data local inpath ‘/home/centos/customers.txt’ into table t4 ; 导入数据load data local inpath ‘/home/zpx/customers.txt’ into table t2 ; //local上传文件load data inpath ‘/user/zpx/customers.txt’ [overwrite] into table t2 //分布式文件系统上移动文件 建视图Hive也可以建立视图，是一张虚表，方便我们进行操作. create view v1 as select a.id aid,a.name ,b.id bid , b.order from customers a left outer join default.tt b on a.id = b.cid ; Hive的严格模式Hive提供了一个严格模式，可以防止用户执行那些产生意想不到的不好的影响的查询。使用了严格模式之后主要对以下3种不良操作进行控制： 1.分区表必须指定分区进行查询。2.order by时必须使用limit子句。3.不允许笛卡尔积。 Hive的动态分区像分区表里面存储了数据。我们在进行存储数据的时候，都是明确的指定了分区。在这个过程中Hive也提供了一种比较任性化的操作，就是动态分区，不需要我们指定分区目录，Hive能够把数据进行动态的分发,我们需要将当前的严格模式设置成非严格模式，否则不允许使用动态分区set hive.exec.dynamic.partition.mode=nonstrict//设置非严格模式 Hive的排序Hive也提供了一些排序的语法，包括order by,sort by。 order by=MapReduce的全排序sort by=MapReduce的部分排序distribute by=MapReduce的分区 selece …….from …… order by 字段；//按照这个字段全排序 selece …….from …… sort by 字段； //按照这个字段局部有序 selece 字段…..from …… distribute by 字段；//按照这个字段分区特别注意的是： 在上面的最后一个distribute by使用过程中，按照排序的字段要出现在最左侧也就是select中有这个字段，因为我们要告诉MapReduce你要按照哪一个字段分区，当然获取的数据中要出现这个字段了。类似于我们使用group by的用法，字段也必须出现在最左侧，因为数据要包含这个字段，才能按照这个字段分组，至于Hive什么时候会自行的开启MapReduce，那就是在使用聚合的情况下开启，使用select …from ….以及使用分区表的selece ….from……where …..不会开启 distribute by与sort by可以组合使用，但是distribute by要放在前边，因为MapReduce要先分区，后排序，再归并 select 字段a,……..from …….distribute by字段a，sort by字段如果distribute by与sort by使用的字段一样，则可以使用cluster by 字段替代：select 字段a,……..from …….cluster by 字段 函数 show functions; 展示相关函数 desc function split; desc function extended split; //查看函数的扩展信息 用户自定义函数（UDF）具体步骤如下： （1）.自定义类（继承UDF，或是GenericUDF。GenericUDF是更为复杂的抽象概念，但是其支持更好的null值处理同时还可以处理一些标准的UDF无法支持的编程操作）。（2）.导出jar包，通过命令添加到hive的类路径。$hive&gt;add jar xxx.jar（3）.注册函数$hive&gt;CREATE TEMPORARY FUNCTION 函数名 AS ‘具体类路径：包.类’;（4）.使用 $hive&gt;select 函数名(参数);自定义实现类如下(继承UDF)：]]></content>
      <tags>
        <tag>bigdata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase积累.md]]></title>
    <url>%2F2018%2F06%2F04%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fhbase%E7%A7%AF%E7%B4%AF%2F</url>
    <content type="text"><![CDATA[hbase积累 细节点1.Rowkey设计原则 1.1 长度原则 rowkey 在hbase以二进制码流,可以是任意字符串, 最大长度是64kb,实际应用主要是100~100bytes 长度尽量为8的整数倍,因为现在的系统主要是64位,内存8字节对齐.控制在16字节,符合操作系统特性 1.2 散列原则:因为hbase是分布式存储,rowkey的高位尽量是散列字段,散列性弱的尽量放在低位段.如Time AND Device_id的组合,相对而言device_id 应该量级较小,散列性高.而TIME散列性低,如果TIME放在高位,可能造成数据在某个RegeionServer上堆积的情况.所以较合理的rowkey组合应是device_id+time. 1.3 RowKey唯一原则：必须在设计上保证其唯一性.hbase 中以KeyValue形式存储,key若重复,行内容则会被覆盖. 2.Hbase的Regeion热点问题解决因为在创建表是没有提前预分区,创建的表默认就只会有一个region,这个region的rowkey是没有边界的,即没有startkey与stopkey.数据在写入时,都会写入到这个region.随着数据的不断增加,达到某个阈值时,才会split成2个region.在这个过程中就会产生所有数据囤积在一个regionServer上,出现热点问题.另在split时,会占用集群的I/O资源.通过预分区可以解决该问题 2.1 预分区预分区,”预”字是核心.我们在建表时,预先对表中要存放的数据形式和可能的量级,心中必然会有所估量,即这里应预估数据量.若数据量较大,则在建表时又应该预分区.即根据数据形式,量级,事先预设好一定量的region,后面数据写入时,则会写入到相应的分区.从而避免热点,减少split. 2.1.2 salting(加盐)hbase rowkey设计,避免热点,常会用到该操作,这里的加盐本身不是加密操作,而是在原数据前加入一些随机数据,从而起到分散不同region的作用. 2.1.3 预习区具体方案 hbase预分区的相关操作,如shell形式,可直接在hbase shell操作.如 https://blog.csdn.net/xiao_jun_0820/article/details/24419793 java形式https://blog.csdn.net/qq_20641565/article/details/56482407 以上操作形式有个问题就是rowkey是随机生成的,虽起到了散列存储,避免了热点堆积,但因为加盐的缘故,想要直接的获取某行数据较为困难.若针对的是高频使用的数据,则会出现问题. 2.1.4 hash分区 在原先预分区的基础上,通过相关规则将原数据hash,从而获得这个原数据对应在哪个分区,使当拿到相关原数据,就能推演出相关rowkey.从而能准确的get数据. hbase优化确定优化目标沟通交流后，业务方更看重降低成本。数据量梳理后略有降低，保证吞吐，无长期请求堆积前提下可以放宽延时要求。为了更快的进行优化，放宽稳定性可以要求接受短期波动。另外，该分组的RegionServer之前存在不稳定的问题，这次优化也一并解决。]]></content>
      <tags>
        <tag>日常总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark学习]]></title>
    <url>%2F2018%2F03%2F04%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fspark%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[spark 学习 1spark 作为主流的实时计算引擎,需要高度掌握 spark介绍Apache Spark是一用于实时处理的开源集群计算框架.持多种语言编程,Spark Streaming有高吞吐量和容错能力强等特点.数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算,而结果也能保存在很多地方，如HDFS，数据库等。另外Spark Streaming也能和MLlib（机器学习）以及Graphx完美融合。 优点 易用 容错 spark体系整合 RDD详解RDD是什么RDD：Spark的核心概念是RDD (resilientdistributed dataset)，指的是一个只读的，可分区的分布式数据集，这个数据集的全部或部分可以缓存在内存中，在多次计算间重用。 另:RDD即弹性分布式数据集，有容错机制并可以被并行操作的元素集合，具有只读、分区、容错、高效、无需物化、可以缓存、RDD依赖等特征。RDD只是数据集的抽象，分区内部并不会存储具体的数据。 RDD的五个特性 有一个分片列表。就是能被切分，和hadoop一样的，能够切分的数据才能并行计算。 有一个函数计算每一个分片，这里指的是下面会提到的compute函数. 对其他的RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖. 可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。 可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。(存储的是一个表，可以将处理的分区“本地化”). 12345678910//只计算一次 protected def getPartitions: Array[Partition] //对一个分片进行计算，得出一个可遍历的结果 def compute(split: Partition, context: TaskContext): Iterator[T] //只计算一次，计算RDD对父RDD的依赖 protected def getDependencies: Seq[Dependency[_]] = deps //可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce @transient val partitioner: Option[Partitioner] = None //可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置 protected def getPreferredLocations(split: Partition): Seq[String] = Nil 为什么会产生RDDRDD数据集 并行集合 接收一个已经存在的集合,然后进行各种并行计算.并行化集合是通过调用SparkContext的parallelize方法，在一个已经存在的Scala集合上创建（一个Seq对象）。集合的对象将会被拷贝，创建出一个可以被并行操作的分布式数据集。 Hadoop数据集 Spark可以将任何Hadoop所支持的存储资源转化成RDD，只要文件系统是HDFS，或者Hadoop支持的任意存储系统即可，如本地文件（需要网络文件系统，所有的节点都必须能访问到）、HDFS、Cassandra、HBase、Amazon S3等，Spark支持文本文件、SequenceFiles和任何Hadoop InputFormat格式。 此两种类型的RDD都可以通过相同的方式进行操作，从而获得子RDD等一系列拓展，形成lineage血统关系图。 Spark RDD算子 Transformation不触发提交作业，完成作业中间处理过程。 DStream什么是DStreamDiscretized Stream :代表持续性的数据流和经过各种Spark原语操作后的结果数据流,在内部实现上是一系列连续的RDD来表示.每个RDD含有一段时间间隔内的数据,如下图 计算则由spark engine来完成 spark java因为我是主要掌握的语言是java,从效率上来考虑,这里 参考博客https://blog.csdn.net/wangxiaotongfan/article/details/51395769 RDD详解https://blog.csdn.net/zuochang_liu/article/details/81459185 spark streaming学习https://blog.csdn.net/hellozhxy/article/details/81672845 spark java 使用指南https://blog.csdn.net/t1dmzks/article/details/70198430 sparkRDD算子介绍https://blog.csdn.net/wxycx11111/article/details/79123482 sparkRDD入门介绍https://github.com/zhaikaishun/spark_tutorial RDD算子介绍]]></content>
      <tags>
        <tag>学习spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark算子]]></title>
    <url>%2F2018%2F03%2F04%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fspark%E7%AE%97%E5%AD%90%2F</url>
    <content type="text"><![CDATA[spark 算子 123sparkRDD封装的函数方法又称算子,通过这些算子可以对RDD进行相关处理,从而获我们想要的结果,因为可能涉及的算子较多.因此单独开篇进行粒度更细,更集中的总结.总得来讲spark的算子,本就是scala集合的一些高阶用法. Transformation(转换)不触发提交作业，完成作业中间处理过程。 parallelize (并行化)将一个存在的集合，变成一个RDD ,返回的是一个JavaRDD[T] in scala 1sc.parallelize(List("shenzhen", "is a beautiful city")) in java 1JavaRDD&lt;String&gt; javaStringRDD = sc.parallelize(Arrays.asList("shenzhen", "is a beautiful city")); makeRDD只有scala版本的才有makeRDD ,与parallelize类似. textFile调用SparkContext.textFile()方法，从外部存储中读取数据来创建 RDD in scala 1var lines = sc.textFile(inpath) 12// java JavaRDD&lt;String&gt; lines = sc.textFile(inpath); filter对RDD数据进行过滤 map接收一个函数,并将这个函数作用于RDD中的每个元素.RDD 中对应元素的值 map是一对一的关系 flatMap有时候，我们希望对某个元素生成多个元素，实现该功能的操作叫作 flatMap() ,faltMap的函数应用于每一个元素，对于每一个元素返回的是多个元素组成的迭代器 distinct去重,我们生成的RDD可能有重复的元素，使用distinct方法可以去掉重复的元素, 不过此方法涉及到混洗，操作开销很大 union两个RDD进行合并 intersectionRDD1.intersection(RDD2) 返回两个RDD的交集， 并且去重 intersection 需要混洗数据，比较浪费性能 subtractRDD1.subtract(RDD2),返回在RDD1中出现，但是不在RDD2中出现的元素，不去重 cartesiancartesian(RDD2) 返回RDD1和RDD2的笛卡儿积，这个开销非常大 mapToPair将元素该成key-value形式 flatMapToPair差异同mapToPair combineByKey该方法主要针对不同分区的同一key进行元素合并函数操作.需要对pairRDD进行 createCombiner 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过,要么就和之前的某个元素的键相同。如果这是一个新的元素， combineByKey() 会使用一个叫作 createCombiner() 的函数来创建那个键对应的累加器的初始值 mergeValue 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并 mergeCombiners 于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。reduceByKey接收一个函数，按照相同的key进行reduce操foldByKey该函数用于RDD[K,V]根据K将V做折叠、合并处理，其中的参数zeroValue表示先根据映射函数将zeroValue应用于V,进行初始化V,再将映射函数应用于初始化后的V ,与reduce不同的是 foldByKey开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素 sortByKeySortByKey用于对pairRDD按照key进行排序，第一个参数可以设置true或者false，默认是true groupByKeygroupByKey会将RDD[key,value] 按照相同的key进行分组，形成RDD[key,Iterable[value]]的形式， 有点类似于sql中的groupby，例如类似于mysql中的group_concat cogroupgroupByKey是对单个 RDD 的数据进行分组，还可以使用一个叫作 cogroup() 的函数对多个共享同一个键的 RDD 进行分组RDD1.cogroup(RDD2) 会将RDD1和RDD2按照相同的key进行分组，得到(key,RDD[key,Iterable[value1],Iterable[value2]])的形式 subtractByKey类似于subtrac，删掉 RDD 中键与 other RDD 中的键相同的元素join可以把RDD1,RDD2中的相同的key给连接起来，类似于sql中的join操作RDD1.join(RDD2) fullOuterJoin全连接leftOuterJoinrightOuterJoin Actionfirst返回第一个元素 takerdd.take(n)返回第n个元素 collectrdd.collect() 返回 RDD 中的所有元素 countrdd.count() 返回 RDD 中的元素个数 countByValue各元素在 RDD 中出现的次数 返回{(key1,次数),(key2,次数),…(keyn,次数)} reduce并行整合RDD中所有数据 fold和 reduce() 一 样， 但是提供了初始值num,每个元素计算时，先要合这个初始值进行折叠, 注意，这里会按照每个分区进行fold，然后分区之间还会再次进行fold toprdd.top(n)按照降序的或者指定的排序规则，返回前n个元素 takeOrderedrdd.take(n)对RDD元素进行升序排序,取出前n个元素并返回，也可以自定义比较器（这里不介绍），类似于top的相反的方法 foreach对 RDD 中的每个元素使用给定的函数 countByKey以RDD{(1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)}为例 rdd.countByKey会返回{(1,1),(2,2),(3,3)} collectAsMap将pair类型(键值对类型)的RDD转换成map, 还是上面的例子 saveAsTextFilesaveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。 saveAsSequenceFilesaveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。 saveAsObjectFilesaveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。 saveAsHadoopFilesaveAsNewAPIHadoopFilemapPartitionsmapPartitionsWithIndexHashPartitionerRangePartitioner自定义分区]]></content>
      <tags>
        <tag>spark学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqoop记录]]></title>
    <url>%2F2018%2F03%2F04%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fsqoop%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[将Mysql数据导入Hive中 命令:12345678sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true --connect jdbc:mysql://211.159.172.76:3306/solo--username root --password 125323Wkq --table tablename --hive-import --hive-table tablename 整库导入1234sqoop import-all-tables --connect jdbc:mysql://211.159.172.76:3306/ --username root --password 125323Wkq --hive-database solo -m 10 --create-hive-table --fields-terminated-by &quot;\t&quot;--hive-import --hive-database qianyang --hive-overwrite sqoop import-all-tables -Dorg.apache.sqoop.splitter.allow_text_splitter=true –connect jdbc:mysql://211.159.172.76:3306/solo –username root –password 125323Wkq –hive-database blog –create-hive-table –hive-import –hive-overwrite -m 10 单表导入sqoop import –connect jdbc:mysql://211.159.172.76:3306/solo –username root –password 125323Wkq –table b3_solo_article –target-dir /blog/article –hive-import –hive-database blog–fields-terminated-by “\t” –hive-table article –hive-overwrite–m 10 sqoop import –connect jdbc:mysql://211.159.172.76:3306/solo –username root –password 125323Wkq –table b3_solo_article –target-dir /blog/article –hive-import –hive-database blog –create-hive-table –hive-table article –hive-overwrite -m 1]]></content>
      <tags>
        <tag>日常总结</tag>
      </tags>
  </entry>
</search>
