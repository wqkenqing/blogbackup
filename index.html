<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="wong&#39;s bolg">
<meta property="og:url" content="http://www.wqkenqing.ren/index.html">
<meta property="og:site_name" content="wong&#39;s bolg">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="wong&#39;s bolg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.wqkenqing.ren/">





  <title>wong's bolg</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">wong's bolg</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.wqkenqing.ren/2019/03/21/spark实战/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kuiq  Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wong's bolg">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/21/spark实战/" itemprop="url">spark实战</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-21T09:59:44+08:00">
                2019-03-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<h1 id="spark实战"><a href="#spark实战" class="headerlink" title="spark实战"></a>spark实战</h1><h2 id="spark-core"><a href="#spark-core" class="headerlink" title="spark-core"></a>spark-core</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.wqkenqing.ren/2019/03/19/日常总结/mapreduce组件总结/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kuiq  Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wong's bolg">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/19/日常总结/mapreduce组件总结/" itemprop="url">mapreduce组件总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-19T14:59:23+08:00">
                2019-03-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<h1 id="mapreduce组件总结"><a href="#mapreduce组件总结" class="headerlink" title="mapreduce组件总结"></a>mapreduce组件总结</h1><p>相关组件大致有</p>
<ol>
<li>Inputformat</li>
<li>Inputsplit</li>
<li>ReadRecorder</li>
<li>mapper</li>
<li>Combiner</li>
<li>Partioner</li>
<li>Reduce</li>
<li>GroupComparator</li>
<li>Reduce</li>
</ol>
<h1 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a>shuffle</h1><p><img src="http://img.wqkenqing.ren/2019-03-19-15-39-59.png" alt="2019-03-19-15-39-59"><br><img src="http://img.wqkenqing.ren/2019-03-19-16-46-06.png" alt="2019-03-19-16-46-06"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shuffle 被称为mapreduce的核心,一个真正让奇迹发生的地方.但它到底是什么呢?简练的讲,它就是 map out 到 reduce in 这段过程中对数据的处理过程.</span><br></pre></td></tr></table></figure>
<p>shuffle过程中主要发生的操作有,Partion,Sort,spill,merge,copy,sort,merge.(还有可能有combine操作)</p>
<p>具体流程是<br>map out后,Collector 对out后的数据进行处理. 数据将会写入到内存缓冲区,该内存缓冲区的数据达到80%后,会开启一个溢写线程,在磁盘本地创建一个文件.如果reduce设置了多个分区,写入buffer区的数据,会被打上一个分区标记.通过sortAndSpill()方法进行指对数据按分区号,key排序.最后溢出的文件是分区的,按key有序的文件.若buffer区中的20%一直未被填满,buffer写入进程不会断.但若达到100%,Buffer写入进程则会阻塞.并在buffer区中的数据全部spill完后才会再开启. (buffer区的内存默认是100M),spill过程中,若设置过combiner.则会对数据先进行combiner逻辑处理,再将处理后的数据写出</p>
<p>spill完成后则会对本地的spill后的文件进行Merge.即把多个spill后的文件进行合并,并排序.最后会行成一个有序文件</p>
<p>当1个Map Task 完成后,reduce 就会开启copy进程(默认是5个线程).这个过程中会通过http请求去各taskTracker(nodemanager),拉取相应的spill&amp;merge后的文件.<br>当copy完成后,则又会对数据进行merge.这个过程中同样有个类似map shuffle 中的buffer 溢写的阶段. 这个过程同样会触发combiner组件.这里的merge数据源有三种</p>
<ol>
<li>memory to memory</li>
<li>memory to disk</li>
<li>disk   to disk<br>默认1是不开启的.</li>
</ol>
<p>copy phase 完成后,是reduceTask 中的 sort phase<br>即对merge 中的文件继续进行sort and group .</p>
<p>当sort phase 完成.则开启reduce phase .到此shuffle正式完成.</p>
<p>##二次排序</p>
<pre><code>
</code></pre><p>mapreduce 常见的辅助排序</p>
<ol>
<li>partitioner</li>
<li>key的比较Comparator</li>
<li>分组函数Grouping Comparator</li>
</ol>
<h2 id="join"><a href="#join" class="headerlink" title="join"></a>join</h2><p>map join ,semi join ,reduce join</p>
<p>## </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.wqkenqing.ren/2019/03/14/日常总结/准备小结/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kuiq  Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wong's bolg">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/14/日常总结/准备小结/" itemprop="url">准备小结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-14T15:27:08+08:00">
                2019-03-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="准备小结"><a href="#准备小结" class="headerlink" title="准备小结"></a>准备小结</h1><h2 id="hdfs存储机制是怎样的"><a href="#hdfs存储机制是怎样的" class="headerlink" title="hdfs存储机制是怎样的?"></a>hdfs存储机制是怎样的?</h2><p>client端发送写文件请求，namenode检查文件是否存在，如果已存在，直接返回错误信息，否则，发送给client一些可用namenode节点<br>client将文件分块，并行存储到不同节点上datanode上，发送完成后，client同时发送信息给namenode和datanode<br>namenode收到的client信息后，发送确信信息给datanode<br>datanode同时收到namenode和datanode的确认信息后，提交写操作。</p>
<h2 id="hadoop中combiner的作用是什么"><a href="#hadoop中combiner的作用是什么" class="headerlink" title="hadoop中combiner的作用是什么?"></a>hadoop中combiner的作用是什么?</h2><p>当map生成的数据过大时，带宽就成了瓶颈，怎样精简压缩传给Reduce的数据，又不影响最终的结果呢。有一种方法就是使用Combiner，Combiner号称本地的Reduce，Reduce最终的输入，是Combiner的输出。</p>
<h2 id="你们数据库怎么导入hive-的-有没有出现问题"><a href="#你们数据库怎么导入hive-的-有没有出现问题" class="headerlink" title="你们数据库怎么导入hive 的,有没有出现问题"></a>你们数据库怎么导入hive 的,有没有出现问题</h2><p>在导入hive的时候，如果数据库中有blob或者text字段，会报错，解决方案在sqoop笔记中。在将数据由Oracle数据库导入到Hive时，发现带有clob字段的表的数据会错乱，出现一些字段全为NULL的空行。由于在项目中CLOB字段没有实际的分析用途，因此考虑将CLOB字段去掉。</p>
<h2 id="hdfs-site-xml的3个主要属性"><a href="#hdfs-site-xml的3个主要属性" class="headerlink" title="hdfs-site.xml的3个主要属性?"></a>hdfs-site.xml的3个主要属性?</h2><p>dfs.name.dir决定的是元数据存储的路径以及DFS的存储方式(磁盘或是远端)<br>dfs.data.dir决定的是数据存储的路径<br>fs.checkpoint.dir用于第二Namenode</p>
<h2 id="下列哪项通常是集群的最主要瓶颈"><a href="#下列哪项通常是集群的最主要瓶颈" class="headerlink" title="下列哪项通常是集群的最主要瓶颈"></a>下列哪项通常是集群的最主要瓶颈</h2><p>磁盘 IO<br>答案：C 磁盘<br>首先集群的目的是为了节省成本，用廉价的 pc 机，取代小型机及大型机。小型机和大型机有什么特点？<br>1.cpu 处理能力强<br>2.内存够大，所以集群的瓶颈不可能是 a 和 d<br>3.如果是互联网有瓶颈，可以让集群搭建内网。每次写入数据都要通过网络（集群是内网），然后还要写入 3 份数据，所以 IO 就会打折扣。</p>
<h2 id="关于-SecondaryNameNode-哪项是正确的？"><a href="#关于-SecondaryNameNode-哪项是正确的？" class="headerlink" title="关于 SecondaryNameNode 哪项是正确的？"></a>关于 SecondaryNameNode 哪项是正确的？</h2><p>它的目的是帮助 NameNode 合并编辑日志，减少 NameNode 启动时间 </p>
<h2 id="mapreduce的原理"><a href="#mapreduce的原理" class="headerlink" title="mapreduce的原理?"></a>mapreduce的原理?</h2><p>MapReduce采用”分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个分节点共同完成，然后通过整合各个节点的中间结果，<br>得到最终结果。简单地说，MapReduce就是”任务的分解与结果的汇总”。<br>在Hadoop中，用于执行MapReduce任务的机器角色有两个：一个是JobTracker；另一个是TaskTracker，JobTracker是用于调度工作的，TaskTracker<br>是用于执行工作的。一个Hadoop集群中只有一台JobTracker。<br>在分布式计算中，MapReduce框架负责处理了并行编程中分布式存储、工作调度、负载均衡、容错均衡、容错处理以及网络通信等复杂问题，把处理<br>过程高度抽象为两个函数：map和reduce，map负责把任务分解成多个任务，reduce负责把分解后多任务处理的结果汇总起来。<br>需要注意的是，用MapReduce来处理的数据集（或任务）必须具备这样的特点：待处理的数据集可以分解成许多小的数据集，而且每一个小数据集都<br>可以完全并行地进行处理。</p>
<h2 id="HDFS存储的机制"><a href="#HDFS存储的机制" class="headerlink" title="HDFS存储的机制?"></a>HDFS存储的机制?</h2><h3 id="写流程："><a href="#写流程：" class="headerlink" title="写流程："></a>写流程：</h3><p>client链接namenode存数据<br>namenode记录一条数据位置信息（元数据），告诉client存哪。<br>client用hdfs的api将数据块（默认是64M）存储到datanode上。<br>datanode将数据水平备份。并且备份完将反馈client。<br>client通知namenode存储块完毕。<br>namenode将元数据同步到内存中。<br>另一块循环上面的过程。</p>
<h3 id="读流程"><a href="#读流程" class="headerlink" title="读流程"></a>读流程</h3><h2 id="举一个简单的例子说明mapreduce是怎么来运行的"><a href="#举一个简单的例子说明mapreduce是怎么来运行的" class="headerlink" title="举一个简单的例子说明mapreduce是怎么来运行的 ?"></a>举一个简单的例子说明mapreduce是怎么来运行的 ?</h2><p>MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。<br>　　Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。<br>Mapper任务的执行过程详解<br>　　每个Mapper任务是一个Java进程，它会读取HDFS中的文件，解析成很多的键值对，经过我们覆盖的map方法处理后，<br>转换为很多的键值对再输出。整个Mapper任务的处理过程又可以分为以下六个阶段：<br>　　第一阶段是把输入文件按照一定的标准分片(InputSplit)，每个输入片的大小是固定的。默认情况下，输入片(InputSplit)<br>的大小与数据块(Block)的大小是相同的。如果数据块(Block)的大小是默认值128MB，输入文件有两个，一个是32MB，一个是　172MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由　一个Mapper进程处理。这里的三个输入片，会有三个Mapper进程处理。</p>
<p>　　第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键”是每一　行的起始位置(单位是字节)，“值”是本行的文本内容。<br>　　<br>　　第三阶段是调用Mapper类中的map方法。第二阶段中解析出来的每一个键值对，调用一次map方法。如果有1000个键值对，就会　调用1000次map方法。每一次调用map方法会输出零个或者多个键值对。</p>
<p>　　第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份(如北京、上海、　山东等)，那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是Reducer　任务运行的数量。默认只有一个Reducer任务。<br>第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值　对&lt;2,2&gt;、&lt;1,3&gt;、&lt;2,1&gt;，键和值分别是整数。那么排序后的结果是&lt;1,3&gt;、&lt;2,1&gt;、&lt;2,2&gt;。如果有第六阶段，那么进入</p>
<p>第六阶段　如果没有，直接输出到本地的Linux文件中。　第六阶段是对数据进行归约处理，也就是reduce处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。　归约后的数据输出到本地的linxu文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。　Reducer任务的执行过程详解<br>每个Reducer任务是一个java进程。Reducer任务接收Mapper任务的输出，归约处理后写入到HDFS中，可以分为三个阶段：<br>第一阶段是Reducer任务会主动从Mapper任务复制其输出的键值对。Mapper任务可能会有很多，因此Reducer会复制多个Mapper的输出。<br>第二阶段是把复制到Reducer本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。<br>第三阶段是对排序后的键值对调用reduce方法。键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对。<br>最后把这些输出的键值对写入到HDFS文件中。<br>在整个MapReduce程序的开发过程中，我们最大的工作量是覆盖map函数和覆盖reduce函数。</p>
<h2 id="了解hashMap-和hashTable吗介绍下，他们有什么区别。"><a href="#了解hashMap-和hashTable吗介绍下，他们有什么区别。" class="headerlink" title="了解hashMap 和hashTable吗介绍下，他们有什么区别。"></a>了解hashMap 和hashTable吗介绍下，他们有什么区别。</h2><h2 id="为什么重写equals还要重写hashcode"><a href="#为什么重写equals还要重写hashcode" class="headerlink" title="为什么重写equals还要重写hashcode"></a>为什么重写equals还要重写hashcode</h2><p>因为equals比较的是内容是一致.但hashcode</p>
<h2 id="说一下map的分类和常见的情况"><a href="#说一下map的分类和常见的情况" class="headerlink" title="说一下map的分类和常见的情况"></a>说一下map的分类和常见的情况</h2><p> hashmap,hashtable,treemap,LinkedHashMap</p>
<ul>
<li>根据键得到值，因此不允许键重复(重复了覆盖了),但允许值重复<h3 id="Hashmap"><a href="#Hashmap" class="headerlink" title="Hashmap"></a>Hashmap</h3>是一个最常用的Map</li>
<li>它根据键的HashCode值存储数据,根据键可以直接获取它的值，具有很快的访问速度，遍历时，取得数据的顺序是完全随机的</li>
<li>最多只允许一条记录的键为Null;允许多条记录的值为 Null;</li>
<li>HashMap不支持线程的同步，即任一时刻可以有多个线程同时写HashMap;可能会导致数据的不一致。</li>
<li>如果需要同步，可以用 Collections的synchronizedMap方法使HashMap具有同步的能力，或者使用ConcurrentHashMap<h3 id="Hashtable"><a href="#Hashtable" class="headerlink" title="Hashtable"></a>Hashtable</h3>Hashtable与 HashMap类似,它继承自Dictionary类,不同的是:它不允许记录的键或者值为空;</li>
<li>它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了 Hashtable在写入时会比较慢<h3 id="LinkedHashMap"><a href="#LinkedHashMap" class="headerlink" title="LinkedHashMap"></a>LinkedHashMap</h3>是 HashMap 的一个子类，保存了记录的插入顺序，在用 Iterator 遍历 LinkedHashMap 时，先得到的记录肯定是先插入的.<br>也可以在构造时用带参数，按照应用次数排序。在遍历的时候会比 HashMap 慢，不过有种情况例外，当 HashMap 容量很大，实际数据较少时，遍历起来可能会比 LinkedHashMap 慢，因为 LinkedHashMap 的遍历速度只和实际数据有关，和容量无关，而 HashMap 的遍历速度和他的容量有关<h3 id="TreeMap"><a href="#TreeMap" class="headerlink" title="TreeMap"></a>TreeMap</h3>实现 SortMap 接口,能够把它保存的记录根据键排序, 默认是按键值的升序排序，也可以指定排序的比较器，当用 Iterator 遍历 TreeMap 时，得到的记录是排过序的</li>
</ul>
<p>HashMap，链表法存储，entry[]数组，线程不安全，可能死锁 concurrentHashMap，segment数组，每个segent下维护一组entry[]数组，每个segment是一把锁，线程安全 LinkedHashMap</p>
<hr>
<h2 id="Object若不重写hashCode-的话，hashCode-如何计算出来的？"><a href="#Object若不重写hashCode-的话，hashCode-如何计算出来的？" class="headerlink" title="Object若不重写hashCode()的话，hashCode()如何计算出来的？"></a>Object若不重写hashCode()的话，hashCode()如何计算出来的？</h2><p>hashcode采用的是</p>
<h2 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h2><h3 id="1-spark的有几种部署模式，每种模式特点？"><a href="#1-spark的有几种部署模式，每种模式特点？" class="headerlink" title="1. spark的有几种部署模式，每种模式特点？"></a>1. spark的有几种部署模式，每种模式特点？</h3><h4 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h4><p>本地模式分三类</p>
<ul>
<li>local：只启动一个executor</li>
<li>local[k]: 启动k个executor</li>
<li>local[*]：启动跟cpu数目相同的 executor</li>
</ul>
<h3 id="cluster模式"><a href="#cluster模式" class="headerlink" title="cluster模式"></a>cluster模式</h3><p>cluster模式肯定就是运行很多机器上了，但是它又分为以下三种模式，区别在于谁去管理资源调度。（说白了，就好像后勤管家，哪里需要资源，后勤管家要负责调度这些资源）</p>
<h4 id="standalone模式"><a href="#standalone模式" class="headerlink" title="standalone模式"></a>standalone模式</h4><p>分布式部署集群，自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础</p>
<h4 id="Spark-on-yarn模式"><a href="#Spark-on-yarn模式" class="headerlink" title="Spark on yarn模式"></a>Spark on yarn模式</h4><p>分布式部署集群，资源和任务监控交给yarn管理<br>粗粒度资源分配方式，包含cluster和client运行模式<br>cluster 适合生产，driver运行在集群子节点，具有容错功能<br>client 适合调试，dirver运行在客户端</p>
<h3 id="2-Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？"><a href="#2-Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？" class="headerlink" title="2. Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？"></a>2. Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？</h3><h4 id="Spark-core"><a href="#Spark-core" class="headerlink" title="Spark core"></a>Spark core</h4><p>是其它组件的基础，spark的内核<br>主要包含：有向循环图、RDD、Lingage、Cache、broadcast等</p>
<h4 id="SparkStreaming"><a href="#SparkStreaming" class="headerlink" title="SparkStreaming"></a>SparkStreaming</h4><p>是一个对实时数据流进行高通量、容错处理的流式处理系统<br>将流式计算分解成一系列短小的批处理作业</p>
<h4 id="Spark-sql："><a href="#Spark-sql：" class="headerlink" title="Spark sql："></a>Spark sql：</h4><p>能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询</p>
<h4 id="MLBase"><a href="#MLBase" class="headerlink" title="MLBase"></a>MLBase</h4><p>是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低<br>MLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime。</p>
<h4 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h4><p>是Spark中用于图和图并行计算</p>
<h4 id="spark有哪些组件"><a href="#spark有哪些组件" class="headerlink" title="spark有哪些组件"></a>spark有哪些组件</h4><p>master：管理集群和节点，不参与计算。<br>worker：计算节点，进程本身不参与计算，和master汇报。<br>Driver：运行程序的main方法，创建spark context对象。<br>spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。<br>client：用户提交程序的入口。</p>
<ul>
<li><a href="https://blog.csdn.net/yirenboy/article/details/47441465" target="_blank" rel="noopener">https://blog.csdn.net/yirenboy/article/details/47441465</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.wqkenqing.ren/2019/03/11/日常总结/kafka/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kuiq  Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wong's bolg">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/11/日常总结/kafka/" itemprop="url">kafka</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-11T09:33:43+08:00">
                2019-03-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Kafka相关小结"><a href="#Kafka相关小结" class="headerlink" title="Kafka相关小结"></a>Kafka相关小结</h1><h2 id="kafka-相关指令"><a href="#kafka-相关指令" class="headerlink" title="kafka 相关指令"></a>kafka 相关指令</h2><p>kafka-server-start.sh config/server.properties &amp; 启动<br>kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic topic_name  :创建topic<br>kafka-console-producer.sh –broker-list localhost:9092 –topic topic_name :创建生产者</p>
<p>kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic topic_name :创建消费者</p>
<p>kafka-console-producer.sh –broker-list namenode:9092 –topic sparkstreaming</p>
<h2 id="kafka-java-api"><a href="#kafka-java-api" class="headerlink" title="kafka java api"></a>kafka java api</h2><p>kafka 虽然搭建较为简单,但想要对针它编程体验还是有些问题.初步使用下来明显感觉对版本的强约束性.以我线上版本<br><img src="http://img.wqkenqing.ren/2019-03-12-11-02-43.png" alt="2019-03-12-11-02-43">为例,我java项目对应的版本则是<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;kafka_2.10&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;0.8.1&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;0.8.2.1&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>
<p>以上版本搭配经由我亲测通过</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.wqkenqing.ren/2019/03/05/日常总结/spark算子/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kuiq  Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wong's bolg">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/05/日常总结/spark算子/" itemprop="url">spark算子</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-05T10:51:40+08:00">
                2019-03-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<h1 id="spark-算子"><a href="#spark-算子" class="headerlink" title="spark 算子"></a>spark 算子</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sparkRDD封装的函数方法又称算子,通过这些算子可以对RDD进行相关处理,从而获我们想要的结果,因为可能涉及的算子较多.因此单独开篇进行粒度更细,更集中的总结.</span><br><span class="line"></span><br><span class="line">总得来讲spark的算子,本就是scala集合的一些高阶用法.</span><br></pre></td></tr></table></figure>
<h2 id="Transformation-转换"><a href="#Transformation-转换" class="headerlink" title="Transformation(转换)"></a>Transformation(转换)</h2><p>不触发提交作业，完成作业中间处理过程。</p>
<h3 id="parallelize-并行化"><a href="#parallelize-并行化" class="headerlink" title="parallelize (并行化)"></a>parallelize (并行化)</h3><p>将一个存在的集合，变成一个RDD ,返回的是一个JavaRDD[T]<br><strong> in scala </strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(<span class="type">List</span>(<span class="string">"shenzhen"</span>, <span class="string">"is a beautiful city"</span>))</span><br></pre></td></tr></table></figure></p>
<p> <strong> in java </strong><br> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; javaStringRDD = sc.parallelize(Arrays.asList(<span class="string">"shenzhen"</span>, <span class="string">"is a beautiful city"</span>));</span><br></pre></td></tr></table></figure></p>
<h3 id="makeRDD"><a href="#makeRDD" class="headerlink" title="makeRDD"></a>makeRDD</h3><p>只有scala版本的才有makeRDD ,与parallelize类似.</p>
<h3 id="textFile"><a href="#textFile" class="headerlink" title="textFile"></a>textFile</h3><p>调用SparkContext.textFile()方法，从外部存储中读取数据来创建 RDD<br><strong> in scala </strong><br> <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> lines = sc.textFile(inpath)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// java</span></span><br><span class="line"> JavaRDD&lt;String&gt; lines = sc.textFile(inpath);</span><br></pre></td></tr></table></figure>
<h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><p>对RDD数据进行过滤</p>
<h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><p>接收一个函数,并将这个函数作用于RDD中的每个元素.RDD 中对应<strong>元素的值 map是一对一的关系 </strong></p>
<h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><p>有时候，我们希望对某个元素生成多个元素，实现该功能的操作叫作 flatMap() ,faltMap的函数应用于每一个元素，对于每一个元素返回的是多个元素组成的迭代器</p>
<h3 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h3><p>去重,我们生成的RDD可能有重复的元素，使用distinct方法可以去掉重复的元素, 不过此方法涉及到混洗，操作开销很大 </p>
<h3 id="union"><a href="#union" class="headerlink" title="union"></a>union</h3><p>两个RDD进行合并 </p>
<h3 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h3><p>RDD1.intersection(RDD2) 返回两个RDD的交集，<strong> 并且去重 </strong><br>intersection 需要混洗数据，比较浪费性能</p>
<h3 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h3><p>RDD1.subtract(RDD2),返回在RDD1中出现，但是不在RDD2中出现的元素，不去重 </p>
<h3 id="cartesian"><a href="#cartesian" class="headerlink" title="cartesian"></a>cartesian</h3><p>cartesian(RDD2) 返回RDD1和RDD2的笛卡儿积，这个开销非常大</p>
<h3 id="mapToPair"><a href="#mapToPair" class="headerlink" title="mapToPair"></a>mapToPair</h3><p>将元素该成key-value形式</p>
<h3 id="flatMapToPair"><a href="#flatMapToPair" class="headerlink" title="flatMapToPair"></a>flatMapToPair</h3><p>差异同mapToPair</p>
<h3 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h3><p>该方法主要针对不同分区的同一key进行元素合并函数操作.<br>需要对pairRDD进行</p>
<ol>
<li>createCombiner  会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过,要么就<br>和之前的某个元素的键相同。如果这是一个新的元素， combineByKey() 会使用一个叫作 createCombiner() 的函数来创建<br>那个键对应的累加器的初始值</li>
<li>mergeValue 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并</li>
<li>mergeCombiners 于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更<br>多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各<br>个分区的结果进行合并。<h3 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h3>接收一个函数，按照相同的key进行reduce操<h3 id="foldByKey"><a href="#foldByKey" class="headerlink" title="foldByKey"></a>foldByKey</h3>该函数用于RDD[K,V]根据K将V做折叠、合并处理，其中的参数zeroValue表示先根据映射函数将zeroValue应用于V,进行初始化V,再将映射函数应用于初始化后的V ,与reduce不同的是 foldByKey开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素 <h3 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h3>SortByKey用于对pairRDD按照key进行排序，第一个参数可以设置true或者false，默认是true <h3 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h3>groupByKey会将RDD[key,value] 按照相同的key进行分组，形成RDD[key,Iterable[value]]的形式， 有点类似于sql中的groupby，例如类似于mysql中的group_concat <h3 id="cogroup"><a href="#cogroup" class="headerlink" title="cogroup"></a>cogroup</h3>groupByKey是对单个 RDD 的数据进行分组，还可以使用一个叫作 cogroup() 的函数对多个共享同一个键的 RDD 进行分组<br>RDD1.cogroup(RDD2) 会将RDD1和RDD2按照相同的key进行分组，得到(key,RDD[key,Iterable[value1],Iterable[value2]])的形式 <h3 id="subtractByKey"><a href="#subtractByKey" class="headerlink" title="subtractByKey"></a>subtractByKey</h3>类似于subtrac，删掉 RDD 中键与 other RDD 中的键相同的元素<h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3>可以把RDD1,RDD2中的相同的key给连接起来，类似于sql中的join操作<br>RDD1.join(RDD2) <h3 id="fullOuterJoin"><a href="#fullOuterJoin" class="headerlink" title="fullOuterJoin"></a>fullOuterJoin</h3>全连接<h3 id="leftOuterJoin"><a href="#leftOuterJoin" class="headerlink" title="leftOuterJoin"></a>leftOuterJoin</h3><h3 id="rightOuterJoin"><a href="#rightOuterJoin" class="headerlink" title="rightOuterJoin"></a>rightOuterJoin</h3></li>
</ol>
<h2 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h2><h3 id="first"><a href="#first" class="headerlink" title="first"></a>first</h3><p>返回第一个元素 </p>
<h3 id="take"><a href="#take" class="headerlink" title="take"></a>take</h3><p>rdd.take(n)返回第n个元素 </p>
<h3 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h3><p>rdd.collect() 返回 RDD 中的所有元素 </p>
<h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><p>rdd.count() 返回 RDD 中的元素个数 </p>
<h3 id="countByValue"><a href="#countByValue" class="headerlink" title="countByValue"></a>countByValue</h3><p>各元素在 RDD 中出现的次数 返回{(key1,次数),(key2,次数),…(keyn,次数)} </p>
<h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h3><p>并行整合RDD中所有数据</p>
<h3 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h3><p>和 reduce() 一 样， 但是提供了初始值num,每个元素计算时，先要合这个初始值进行折叠, 注意，这里会按照每个分区进行fold，然后分区之间还会再次进行fold </p>
<h3 id="top"><a href="#top" class="headerlink" title="top"></a>top</h3><p>rdd.top(n)<br>按照降序的或者指定的排序规则，返回前n个元素 </p>
<h3 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered"></a>takeOrdered</h3><p>rdd.take(n)<br>对RDD元素进行升序排序,取出前n个元素并返回，也可以自定义比较器（这里不介绍），类似于top的相反的方法 </p>
<h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h3><p>对 RDD 中的每个元素使用给<br>定的函数</p>
<h3 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h3><p>以RDD{(1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)}为例 rdd.countByKey会返回{(1,1),(2,2),(3,3)} </p>
<h3 id="collectAsMap"><a href="#collectAsMap" class="headerlink" title="collectAsMap"></a>collectAsMap</h3><p>将pair类型(键值对类型)的RDD转换成map, 还是上面的例子</p>
<h3 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile"></a>saveAsTextFile</h3><p>saveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。</p>
<h3 id="saveAsSequenceFile"><a href="#saveAsSequenceFile" class="headerlink" title="saveAsSequenceFile"></a>saveAsSequenceFile</h3><p>saveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。</p>
<h3 id="saveAsObjectFile"><a href="#saveAsObjectFile" class="headerlink" title="saveAsObjectFile"></a>saveAsObjectFile</h3><p>saveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。</p>
<h3 id="saveAsHadoopFile"><a href="#saveAsHadoopFile" class="headerlink" title="saveAsHadoopFile"></a>saveAsHadoopFile</h3><h3 id="saveAsNewAPIHadoopFile"><a href="#saveAsNewAPIHadoopFile" class="headerlink" title="saveAsNewAPIHadoopFile"></a>saveAsNewAPIHadoopFile</h3><h3 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h3><h3 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h3><h3 id="HashPartitioner"><a href="#HashPartitioner" class="headerlink" title="HashPartitioner"></a>HashPartitioner</h3><h3 id="RangePartitioner"><a href="#RangePartitioner" class="headerlink" title="RangePartitioner"></a>RangePartitioner</h3><h3 id="自定义分区"><a href="#自定义分区" class="headerlink" title="自定义分区"></a>自定义分区</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.wqkenqing.ren/2019/03/04/日常总结/spark学习/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kuiq  Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wong's bolg">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/04/日常总结/spark学习/" itemprop="url">spark学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-04T11:12:58+08:00">
                2019-03-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<h1 id="spark-学习"><a href="#spark-学习" class="headerlink" title="spark 学习"></a>spark 学习</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark 作为主流的实时计算引擎,需要高度掌握</span><br></pre></td></tr></table></figure>
<h2 id="spark介绍"><a href="#spark介绍" class="headerlink" title="spark介绍"></a>spark介绍</h2><p>Apache Spark是一用于实时处理的开源集群计算框架.持多种语言编程,Spark Streaming有高吞吐量和容错能力强等特点.<br>数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算,而结果也能保存在很多地方，如HDFS，数据库等。另外Spark Streaming也能和MLlib（机器学习）以及Graphx完美融合。</p>
<p>优点</p>
<ul>
<li>易用</li>
<li>容错</li>
<li>spark体系整合</li>
</ul>
<p><img src="http://img.wqkenqing.ren/2019-03-04-15-45-38.png" alt="spark&amp;storm对比"></p>
<h2 id="RDD详解"><a href="#RDD详解" class="headerlink" title="RDD详解"></a>RDD详解</h2><h3 id="RDD是什么"><a href="#RDD是什么" class="headerlink" title="RDD是什么"></a>RDD是什么</h3><p>RDD：Spark的核心概念是RDD (resilientdistributed dataset)，指的是一个只读的，可分区的分布式数据集，这个数据集的全部或部分可以缓存在内存中，在多次计算间重用。</p>
<p>另:RDD即弹性分布式数据集，有容错机制并可以被并行操作的元素集合，具有只读、分区、容错、高效、无需物化、可以缓存、RDD依赖等特征。RDD只是数据集的抽象，分区内部并不会存储具体的数据。</p>
<p>RDD的五个特性</p>
<ol>
<li>有一个分片列表。就是能被切分，和hadoop一样的，能够切分的数据才能并行计算。 </li>
<li>有一个函数计算每一个分片，这里指的是下面会提到的compute函数.</li>
<li>对其他的RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖.</li>
<li>可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。</li>
<li>可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。(存储的是一个表，可以将处理的分区“本地化”).</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//只计算一次  </span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>]  </span><br><span class="line">  <span class="comment">//对一个分片进行计算，得出一个可遍历的结果</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]</span><br><span class="line">  <span class="comment">//只计算一次，计算RDD对父RDD的依赖</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = deps</span><br><span class="line">  <span class="comment">//可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce</span></span><br><span class="line">  <span class="meta">@transient</span> <span class="keyword">val</span> partitioner: <span class="type">Option</span>[<span class="type">Partitioner</span>] = <span class="type">None</span></span><br><span class="line">  <span class="comment">//可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocations</span></span>(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span></span><br></pre></td></tr></table></figure>
<h3 id="为什么会产生RDD"><a href="#为什么会产生RDD" class="headerlink" title="为什么会产生RDD"></a>为什么会产生RDD</h3><h3 id="RDD数据集"><a href="#RDD数据集" class="headerlink" title="RDD数据集"></a>RDD数据集</h3><ol>
<li>并行集合</li>
</ol>
<p>接收一个已经存在的集合,然后进行各种并行计算.并行化集合是通过调用SparkContext的parallelize方法，在一个已经存在的Scala集合上创建（一个Seq对象）。集合的对象将会被拷贝，创建出一个可以被并行操作的分布式数据集。</p>
<ol start="2">
<li>Hadoop数据集</li>
</ol>
<p>Spark可以将任何Hadoop所支持的存储资源转化成RDD，只要文件系统是HDFS，或者Hadoop支持的任意存储系统即可，如本地文件（需要网络文件系统，所有的节点都必须能访问到）、HDFS、Cassandra、HBase、Amazon S3等，Spark支持文本文件、SequenceFiles和任何Hadoop InputFormat格式。</p>
<p>此两种类型的RDD都可以通过相同的方式进行操作，从而获得子RDD等一系列拓展，形成lineage血统关系图。</p>
<h3 id="Spark-RDD算子"><a href="#Spark-RDD算子" class="headerlink" title="Spark RDD算子"></a>Spark RDD算子</h3><ol>
<li>Transformation<br>不触发提交作业，完成作业中间处理过程。</li>
</ol>
<h2 id="DStream"><a href="#DStream" class="headerlink" title="DStream"></a>DStream</h2><h3 id="什么是DStream"><a href="#什么是DStream" class="headerlink" title="什么是DStream"></a>什么是DStream</h3><p>Discretized Stream :代表持续性的数据流和经过各种Spark原语操作后的结果数据流,在内部实现上是一系列连续的RDD来表示.每个RDD含有一段时间间隔内的数据,如下图<br><img src="http://img.wqkenqing.ren/2019-03-04-15-50-41.png" alt="DStream"></p>
<p>计算则由spark engine来完成<br><img src="http://img.wqkenqing.ren/2019-03-04-15-51-58.png" alt="spark engine流程"></p>
<h2 id="spark-java"><a href="#spark-java" class="headerlink" title="spark java"></a>spark java</h2><p>因为我是主要掌握的语言是java,从效率上来考虑,这里</p>
<h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><p><a href="https://blog.csdn.net/wangxiaotongfan/article/details/51395769" target="_blank" rel="noopener">https://blog.csdn.net/wangxiaotongfan/article/details/51395769</a> RDD详解<br><a href="https://blog.csdn.net/zuochang_liu/article/details/81459185" target="_blank" rel="noopener">https://blog.csdn.net/zuochang_liu/article/details/81459185</a>  spark streaming学习<br><a href="https://blog.csdn.net/hellozhxy/article/details/81672845" target="_blank" rel="noopener">https://blog.csdn.net/hellozhxy/article/details/81672845</a> spark java 使用指南<br><a href="https://blog.csdn.net/t1dmzks/article/details/70198430" target="_blank" rel="noopener">https://blog.csdn.net/t1dmzks/article/details/70198430</a> sparkRDD算子介绍<br><a href="https://blog.csdn.net/wxycx11111/article/details/79123482" target="_blank" rel="noopener">https://blog.csdn.net/wxycx11111/article/details/79123482</a> <strong>sparkRDD入门介绍</strong><br><a href="https://github.com/zhaikaishun/spark_tutorial" target="_blank" rel="noopener">https://github.com/zhaikaishun/spark_tutorial</a> <strong>RDD算子介绍</strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.wqkenqing.ren/2019/03/04/日常总结/hbase积累/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kuiq  Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wong's bolg">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/04/日常总结/hbase积累/" itemprop="url">hbase积累.md</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-04T10:54:48+08:00">
                2019-03-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<h1 id="hbase积累"><a href="#hbase积累" class="headerlink" title="hbase积累"></a>hbase积累</h1><h2 id="细节点"><a href="#细节点" class="headerlink" title="细节点"></a>细节点</h2><h3 id="1-Rowkey设计原则"><a href="#1-Rowkey设计原则" class="headerlink" title="1.Rowkey设计原则"></a>1.Rowkey设计原则</h3><p>1.1 <strong>长度原则</strong> rowkey 在hbase以二进制码流,可以是任意字符串,</p>
<ul>
<li><strong>最大长度是64kb</strong>,实际应用主要是100~100bytes</li>
<li>长度尽量为8的整数倍,因为现在的系统主要是64位,内存8字节对齐.控制在16字节,符合操作系统特性</li>
</ul>
<p>1.2 <strong>散列原则</strong>:因为hbase是分布式存储,rowkey的高位尽量是散列字段,散列性弱的尽量放在低位段.如Time AND Device_id的组合,相对而言device_id 应该量级较小,散列性高.而TIME散列性低,如果TIME放在高位,可能造成数据在某个RegeionServer上堆积的情况.所以较合理的rowkey组合应是<br>device_id+time.</p>
<p>1.3 <strong>RowKey唯一原则</strong>：必须在设计上保证其唯一性.<br>hbase 中以KeyValue形式存储,key若重复,行内容则会被覆盖.</p>
<hr>
<h3 id="2-Hbase的Regeion热点问题解决"><a href="#2-Hbase的Regeion热点问题解决" class="headerlink" title="2.Hbase的Regeion热点问题解决"></a>2.Hbase的Regeion热点问题解决</h3><p><code>因为在创建表是没有提前预分区,创建的表默认就只会有一个region,这个region的rowkey是没有边界的,即没有startkey与stopkey.数据在写入时,都会写入到这个region.随着数据的不断增加,达到某个阈值时,才会split成2个region.在这个过程中就会产生所有数据囤积在一个regionServer上,出现热点问题.另在split时,会占用集群的I/O资源.通过预分区可以解决该问题</code></p>
<h4 id="2-1-预分区"><a href="#2-1-预分区" class="headerlink" title="2.1 预分区"></a>2.1 预分区</h4><p>预分区,”预”字是核心.我们在建表时,预先对表中要存放的数据形式和可能的量级,心中必然会有所估量,即这里应<strong>预</strong>估数据量.若数据量较大,则在建表时又应该预分区.即根据数据形式,量级,事先预设好一定量的region,后面数据写入时,则会写入到相应的分区.从而避免热点,减少split.</p>
<p>2.1.2 salting(加盐)<br>hbase rowkey设计,避免热点,常会用到该操作,这里的加盐本身不是加密操作,而是在原数据前加入一些随机数据,从而起到分散不同region的作用.</p>
<p>2.1.3 预习区具体方案</p>
<p>hbase预分区的相关操作,如shell形式,可直接在hbase shell<br>操作.如</p>
<p><a href="Hbase shell 预分区操作.">https://blog.csdn.net/xiao_jun_0820/article/details/24419793</a></p>
<p>java形式<br><a href="Hbase 预分区 java API形式">https://blog.csdn.net/qq_20641565/article/details/56482407</a></p>
<p>以上操作形式有个问题就是rowkey是随机生成的,虽起到了散列存储,避免了热点堆积,但因为加盐的缘故,想要直接的获取某行数据较为困难.若针对的是高频使用的数据,则会出现问题.</p>
<p>2.1.4 hash分区</p>
<p>在原先预分区的基础上,通过相关规则将原数据hash,从而获得这个原数据对应在哪个分区,使当拿到相关原数据,就能推演出相关rowkey.从而能准确的get数据.</p>
<h3 id="hbase优化"><a href="#hbase优化" class="headerlink" title="hbase优化"></a>hbase优化</h3><h4 id="确定优化目标"><a href="#确定优化目标" class="headerlink" title="确定优化目标"></a>确定优化目标</h4><p>沟通交流后，业务方更看重降低成本。数据量梳理后略有降低，保证吞吐，无长期请求堆积前提下可以放宽延时要求。为了更快的进行优化，放宽稳定性可以要求接受短期波动。<br>另外，该分组的RegionServer之前存在不稳定的问题，这次优化也一并解决。</p>
<hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.wqkenqing.ren/2018/12/24/日常总结/hive总结/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kuiq  Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wong's bolg">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/24/日常总结/hive总结/" itemprop="url">hive总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-24T23:07:45+08:00">
                2018-12-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<h1 id="Hive相关点小结"><a href="#Hive相关点小结" class="headerlink" title="Hive相关点小结"></a>Hive相关点小结</h1><h2 id="启动指令"><a href="#启动指令" class="headerlink" title="启动指令"></a>启动指令</h2><ol>
<li>hive ==  hive –service cli<br>不需要启动server，使用本地的metastore，可以直接做一些简单的数据操作和测试。</li>
<li>启动hiveserver2<br>hive –service hiveserver2</li>
<li>beeline工具测试使用jdbc方式连接<br>beeline -u jdbc:hive2://localhost:10000</li>
</ol>
<p>1.managed table<br>管理表。<br>删除表时，数据也删除了</p>
<p>2.external table<br>外部表。<br>删除表时，数据不删</p>
<h2 id="建表"><a href="#建表" class="headerlink" title="建表:"></a>建表:</h2><p>CREATE TABLE IF NOT EXISTS t2(id int,name string,age int)<br>COMMENT ‘xx’                                     //注释<br>ROW FORMAT DELIMITED                             //行分隔符<br>FIELDS TERMINATED BY ‘,’                         //字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改<br>STORED AS TEXTFILE ;</p>
<h3 id="外部表"><a href="#外部表" class="headerlink" title="外部表:"></a>外部表:</h3><p> CREATE  TABLE IF NOT EXISTS t2(id int,name string,age int)<br> COMMENT ‘xx’<br> ROW FORMAT DELIMITED<br> FIELDS TERMINATED BY ‘,’<br> STORED AS TEXTFILE ; </p>
<h3 id="分区表，桶表"><a href="#分区表，桶表" class="headerlink" title="分区表，桶表"></a>分区表，桶表</h3><h4 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h4><p>Hive中有分区表的概念。我们可以看到分区表具有重要的性能，而且分区表还可以将数据以一种符合逻辑的方式进行组织，比如分层存储。Hive的分区表，是把数据放在满足条件的分区目录下<br>CREATE TABLE t3(id int,name string,age int) </p>
<p>PARTITIONED BY (Year INT, Month INT)   //按照年月进行分区</p>
<p> ROW FORMAT DELIMITED                      //行分隔符</p>
<p>FIELDS TERMINATED BY ‘,’ ;                    //字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改<br>load data local inpath ‘/home/zpx/customers.txt’ into table t3 partition</p>
<h4 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h4><p>这样做，在查找数据的时候就可以跨越多个桶，直接查找复合条件的数据了。速度快，时间成本低。Hive中的桶表默认使用的机制也是hash。<br>CREATE TABLE t4(id int,name string,age int) </p>
<pre><code>CLUSTERED BY (id) INTO 3 BUCKETS      //创建3个通桶表，按照字段id进行分桶

ROW FORMAT DELIMITED                     //行分隔符

FIELDS TERMINATED BY &apos;,&apos; ; 
</code></pre><p>load data local inpath ‘/home/centos/customers.txt’ into table t4 ;</p>
<h2 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h2><p>load data local inpath ‘/home/zpx/customers.txt’ into table t2 ; //local上传文件<br>load data inpath ‘/user/zpx/customers.txt’ [overwrite] into table t2 //分布式文件系统上移动文件</p>
<h2 id="建视图"><a href="#建视图" class="headerlink" title="建视图"></a>建视图</h2><p>Hive也可以建立视图，是一张虚表，方便我们进行操作.</p>
<p>create view v1 as select a.id aid,a.name ,b.id bid , b.order from customers a left outer join default.tt b on a.id = b.cid ;</p>
<h2 id="Hive的严格模式"><a href="#Hive的严格模式" class="headerlink" title="Hive的严格模式"></a>Hive的严格模式</h2><p>Hive提供了一个严格模式，可以防止用户执行那些产生意想不到的不好的影响的查询。<br>使用了严格模式之后主要对以下3种不良操作进行控制：</p>
<p>1.分区表必须指定分区进行查询。<br>2.order by时必须使用limit子句。<br>3.不允许笛卡尔积。<br><img src="http://img.wqkenqing.ren/2019-03-18-17-13-36.png" alt="2019-03-18-17-13-36"></p>
<h2 id="Hive的动态分区"><a href="#Hive的动态分区" class="headerlink" title="Hive的动态分区"></a>Hive的动态分区</h2><p>像分区表里面存储了数据。我们在进行存储数据的时候，都是明确的指定了分区。在这个过程中Hive也提供了一种比较任性化的操作，就是动态分区，不需要我们指定分区目录，Hive能够把数据进行动态的分发,<strong>我们需要将当前的严格模式设置成非严格模式，否则不允许使用动态分区</strong><br>set hive.exec.dynamic.partition.mode=nonstrict//设置非严格模式</p>
<h2 id="Hive的排序"><a href="#Hive的排序" class="headerlink" title="Hive的排序"></a>Hive的排序</h2><p>Hive也提供了一些排序的语法，包括order by,sort by。</p>
<p>order by=MapReduce的全排序<br>sort by=MapReduce的部分排序<br>distribute by=MapReduce的分区</p>
<p>selece …….from …… order by 字段；//按照这个字段全排序</p>
<p>selece …….from …… sort by 字段； //按照这个字段局部有序</p>
<p>selece 字段…..from …… distribute by 字段；//按照这个字段分区<br>特别注意的是：</p>
<ol>
<li>在上面的最后一个distribute by使用过程中，按照排序的字段要出现在最左侧也就是select中有这个字段，因为我们要告诉MapReduce你要按照哪一个字段分区，当然获取的数据中要出现这个字段了。类似于我们使用group by的用法，字段也必须出现在最左侧，因为数据要包含这个字段，才能按照这个字段分组，至于Hive什么时候会自行的开启MapReduce，那就是在使用聚合的情况下开启，使用select …from ….以及使用分区表的selece ….from……where …..不会开启</li>
<li>distribute by与sort by可以组合使用，但是distribute by要放在前边，因为MapReduce要先分区，后排序，再归并</li>
</ol>
<p>select 字段a,……..from …….distribute by字段a，sort by字段<br>如果distribute by与sort by使用的字段一样，则可以使用cluster by 字段替代：<br>select 字段a,……..from …….cluster by 字段</p>
<h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><ol>
<li>show functions; 展示相关函数</li>
<li>desc function split;</li>
<li>desc function  extended split;  //查看函数的扩展信息</li>
</ol>
<h3 id="用户自定义函数（UDF）"><a href="#用户自定义函数（UDF）" class="headerlink" title="用户自定义函数（UDF）"></a>用户自定义函数（UDF）</h3><p>具体步骤如下：</p>
<p>（1）.自定义类（继承UDF，或是GenericUDF。GenericUDF是更为复杂的抽象概念，但是其支持更好的null值处理同时还可以处理一些标准的UDF无法支持的编程操作）。<br>（2）.导出jar包，通过命令添加到hive的类路径。<br>$hive&gt;add jar xxx.jar<br>（3）.注册函数<br>$hive&gt;CREATE TEMPORARY FUNCTION 函数名 AS ‘具体类路径：包.类’;<br>（4）.使用<br> $hive&gt;select 函数名(参数);<br>自定义实现类如下(继承UDF)：</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Kuiq  Wang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kuiq  Wang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
