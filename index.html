<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="wong&#39;s bolg">
<meta property="og:url" content="http://www.wqkenqing.ren/index.html">
<meta property="og:site_name" content="wong&#39;s bolg">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="wong&#39;s bolg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.wqkenqing.ren/">





  <title>wong's bolg</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">wong's bolg</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.wqkenqing.ren/2019/04/15/日常总结/大数据相关分享/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kuiq  Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wong's bolg">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/15/日常总结/大数据相关分享/" itemprop="url">大数据分享</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-15T09:50:34+08:00">
                2019-04-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><code>工欲善其事，必先利其器</code></p>
<h2 id="开头语"><a href="#开头语" class="headerlink" title="开头语"></a>开头语</h2><p>本次分享,是我在公司的第一次分享,我考虑后,将本次分享的主要内容分为了三大部块.先是针对相关基础组件分类介绍.再介绍下通过对这些组件进行组织配搭的大数据基础环境架构.再结合我的一些经历,为大家介绍下相关的应用与产品落地.</p>
<h2 id="技术栈简介"><a href="#技术栈简介" class="headerlink" title="技术栈简介"></a>技术栈简介</h2><ul>
<li>数据采集</li>
<li>数据存储</li>
<li>数据治理(清洗&amp;处理)</li>
<li>数据应用</li>
<li>产品落地</li>
</ul>
<p>我又根据不同组件的特性将他们分</p>
<ul>
<li>采集类</li>
<li>存储类</li>
<li>计算处理类</li>
<li>传输类</li>
<li>管理类 </li>
<li>其它类</li>
</ul>
<p>下面开始具体介绍</p>
<h2 id="采集类"><a href="#采集类" class="headerlink" title="采集类"></a>采集类</h2><p>数据源:</p>
<ul>
<li>日志</li>
<li>业务数据</li>
<li>公网数据(爬虫)</li>
<li>文本数据</li>
<li>出行数据(gps,手机定位等)</li>
</ul>
<p><img src="http://img.wqkenqing.ren/2019-04-15-10-36-53.png" alt="2019-04-15-10-36-53"></p>
<ul>
<li>sqoop flume crawler datax kettle  elk</li>
</ul>
<ol>
<li>Flume(水槽) 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单,可拓展</li>
<li>Sqoop是一个在结构化数据和Hadoop之间进行批量数据迁移的工具，结构化数据可以是Mysql、Oracle等RDBMS。Sqoop底层用MapReduce程序实现抽取、转换、加载，MapReduce天生的特性保证了并行化和高容错率，而且相比Kettle等传统ETL工具，任务跑在Hadoop集群上，减少了ETL服务器资源的使用情况。在特定场景下，抽取过程会有很大的性能提升。</li>
<li>crawler , jsoup ,httpclient, nutch 等.</li>
<li>elk  集中式日志系统 ELK 协议栈详解<br><img src="http://img.wqkenqing.ren/2019-04-15-10-11-16.png" alt="2019-04-15-10-11-16"></li>
</ol>
<hr>
<h2 id="存储类"><a href="#存储类" class="headerlink" title="存储类"></a>存储类</h2><ul>
<li>hdfs</li>
<li>hbase</li>
<li>hive</li>
<li>mongdb</li>
<li>redis</li>
<li>RDBMS</li>
</ul>
<h3 id="hdfs"><a href="#hdfs" class="headerlink" title="hdfs"></a>hdfs</h3><pre><code>* 分布式文件存储系统
* 提供了高可靠性、高扩展性和高吞吐率的数据存储服务
* hdfs典型结构：物理结构+逻辑结构

* 文件线性切割成Block：偏移量（offset）
* Block分散存储在集群节点中
* 单一文件Block大小一致，文件与文件可以不一致
* Block可以设置副本数，副本分散在不同的节点中
* 副本数不要超过节点数量
* 文件上传可以设置Block大小和副本数
* 已上传的文件Block副本数可以调整，大小不变
* 只支持一次写入多次读取，同一时刻只有一个写入者
* 只能追加，不能修改
</code></pre><p><img src="http://img.wqkenqing.ren/2019-04-15-11-12-40.png" alt="2019-04-15-11-12-40"></p>
<h3 id="hbase"><a href="#hbase" class="headerlink" title="hbase"></a>hbase</h3><p>HBase是一个构建在HDFS上的分布式列存储系统；<br>HBase是基于Google BigTable模型开发的，典型的key/value系统；<br>HBase是Apache Hadoop生态系统中的重要一员，主要用于海量结构化数据存储；</p>
<p>大：一个表可以有数十亿行，上百万列；无模式：每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一张表中不同的行可以有截然不同的列；面向列：面向列（族）的存储和权限控制，列（族）独立检索；稀疏：空（null）列并不占用存储空间，表可以设计的非常稀疏；数据多版本：每个单元中的数据可以有多个版本，默认情况下版本号自动分配，是单元格插入时的时间戳；数据类型单一：Hbase中的数据都是字符串，没有类型</p>
<h4 id="openTSDB"><a href="#openTSDB" class="headerlink" title="openTSDB"></a>openTSDB</h4><p>基于Hbase的分布式的，可伸缩的时间序列数据库。<br>主要用途，就是做监控系统；譬如收集大规模集群（包括网络设备、操作系统、应用程序）的监控数据并进行存储，查询。<br><img src="http://img.wqkenqing.ren/2019-04-15-11-27-46.png" alt="2019-04-15-11-27-46"></p>
<h4 id="solr-amp-Phoenix"><a href="#solr-amp-Phoenix" class="headerlink" title="solr &amp; Phoenix"></a>solr &amp; Phoenix</h4><p>二级索引</p>
<h3 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h3><p>Hive 是一个基于 Hadoop 文件系统之上的数据仓库架构。它可以将结构化的数据文件映射为一张数据库表，并提供简单的 sql 查询功能。还可以将 sql 语句转换为 MapReduce 任务运行。<br>底部计算引擎还可以用用Tez, spark等.<br><img src="http://img.wqkenqing.ren/2019-04-15-11-36-43.png" alt="2019-04-15-11-36-43"></p>
<h5 id="Impala"><a href="#Impala" class="headerlink" title="Impala"></a>Impala</h5><p>Impala是Cloudera公司推出，提供对HDFS、Hbase数据的高性能、低延迟的交互式SQL查询功能。</p>
<ul>
<li>基于Hive使用内存计算，兼顾数据仓库、具有实时、批处理、多并发等优点</li>
<li>对内存依赖大,稳定性不如hive</li>
</ul>
<p>相比hive数据仓库,impala针对的量级相关少些,但会有效率的提升.但一般来讲,数据仓库一类需求对时间上的要要求一般不会太高,所以常规方式一般就符合大多数需求.</p>
<h2 id="计算处理类"><a href="#计算处理类" class="headerlink" title="计算处理类"></a>计算处理类</h2><ul>
<li>mapreduce</li>
<li>mapreduce on oozie ,on tez </li>
<li>spark </li>
<li>flink</li>
</ul>
<h3 id="mapreduce"><a href="#mapreduce" class="headerlink" title="mapreduce"></a>mapreduce</h3><p>Mapreduce是一个计算框架，既然是做计算的框架，那么表现形式就是有个输入（input），mapreduce操作这个输（input），通过本身定义好的计算模型，得到一个输出（output），这个输出就是我们所需要的结果。我们要学习的就是这个计算模型的运行规则。在运行一个mapreduce计算任务时候，任务过程被分为两个阶段：map阶段和reduce阶段，每个阶段都是用键值对（key/value）作为输入（input）和输出（output）。而程序员要做的就是定义好这两个阶段的函数：map函数和reduce函数。</p>
<p>分布式计算；<br>移动计算而不移动数据。<br><img src="http://img.wqkenqing.ren/2019-04-15-11-59-56.png" alt="2019-04-15-11-59-56"></p>
<h3 id="spark-storm"><a href="#spark-storm" class="headerlink" title="spark/storm"></a>spark/storm</h3><p>相比一二代计算引擎,在兼并了一二代的特色之外,还引放了流计算这一能力,还丰富了计算函数.<br>其中比较有代表性的主要就是spark&amp;storm.<br>也就是说这代计算引擎兼具无边界数据与有边界数据同样的处理能力.同时还具有DAG特性.<br>这里主要介绍spark</p>
<p>spark主要组成有以下</p>
<ul>
<li>spark-core</li>
<li>spark-streaming</li>
<li>spark-sql</li>
<li>spark-mlib</li>
<li>spark-graphX。</li>
</ul>
<p>spark-core是一个提供内存计算的框架,其他的四大框架都是基于spark core上进行计算的,所以没有spark core,其他的框架是浮云.<br>spark-core的主要内容就是对RDD的操作<br>RDD的创建 -&gt;RDD的转换 -&gt;RDD的缓存 -&gt;RDD的行动 -&gt;RDD的输出</p>
<p>spark-streaming中使用离散化流（discretized stream）作为抽象的表示，叫做DStream。它是随时间推移而收集数据的序列，每个时间段收集到的数据在DStream内部以一个RDD的形式存在。DStream支持从kafka，flume,hdfs,s3等获取输入。DStream也支持两种操作，即转化操作和输出操作</p>
<p>spark-sql<br>Spark SQL 提供了查询结构化数据及计算结果等信息的接口.<br>查询结果以 Datasets and DataFrames 形式返回</p>
<p>…</p>
<h3 id="flink-blink"><a href="#flink-blink" class="headerlink" title="flink/blink"></a>flink/blink</h3><h2 id="传输类"><a href="#传输类" class="headerlink" title="传输类"></a>传输类</h2><h3 id="kafka"><a href="#kafka" class="headerlink" title="kafka"></a>kafka</h3><p>Kafka是分布式发布-订阅消息系统,一个分布式的，可划分的，冗余备份的持久性的日志服务。它主要用于处理活跃的流式数据。日常中常与spark-streaming结合实用,为其提供无边界数据</p>
<p><img src="http://img.wqkenqing.ren/2019-04-16-15-06-25.png" alt="2019-04-16-15-06-25"></p>
<h2 id="管理类-Hue-cloudera-manager"><a href="#管理类-Hue-cloudera-manager" class="headerlink" title="管理类 Hue cloudera-manager"></a>管理类 Hue cloudera-manager</h2><p>hue与cm 都是由cloudera提供,后面cloudera将hue开源给了apache.如果基础集群环境是采用的是开源自主搭建,可考虑引入hue.另一些大数据服务公司,有集成打包自己的一些大数据产品,如cdh等.但这些服务收费,涉及到成本问题.所以如何选用,需要相关斟酌.</p>
<h2 id="其它类-zookeeper-yarn等"><a href="#其它类-zookeeper-yarn等" class="headerlink" title="其它类 zookeeper ,yarn等"></a>其它类 zookeeper ,yarn等</h2><p>zookeeper在集中基础环境中主要作为配置分享中心,与kafka,hbase等组件集成.yarn则作为资源管理组件,可以与mapreduce ,spark等集成</p>
<h2 id="各类组件架构"><a href="#各类组件架构" class="headerlink" title="各类组件架构"></a>各类组件架构</h2><p>以上,已经大致介绍了各类工具,基本了解了相应的特性和使用场景,而根据它们的特性,进行合理的配备,架构,从而实现一个功能全面,稳定的大数据环境.</p>
<p>于我个人经历与平时了解来讲,一般的架构主要如下</p>
<p><img src="http://img.wqkenqing.ren/2019-04-16-10-42-08.png" alt="2019-04-16-10-42-08"></p>
<p>总得来说,各类组件供选型一般来讲都不是单一的.所以,我们的大数据环境各部份组件都是插销式可插拔的.所以不同公司可能不一而同,具体看自身需求和实际情况.比如上图中的storm流式计算模块,就可以替换成spark-streaming等.</p>
<p>通过对上图的架构的拆解,再组合,可能还会有以下组织架构.</p>
<p>数据仓库<br>可以理解为上图中间部份.作为一个数据集市的存在,算作数据中心的一部份.</p>
<ul>
<li>ODS：是数据仓库第一层数据，直接从原始数据过来的，经过简单地处理，比喻：字段体重的数据为175cm等数据。</li>
<li>DW*：这个是数据仓库的第二层数据，DWD和DWS很多情况下是并列存在的，这一层储存经过处理后的标准数据，比喻订单、用户、页面点击流量等数据。</li>
<li>ADS：这个是数据仓库的最后一层数据，为应用层数据，直接可以给业务人员使用。<br><img src="http://img.wqkenqing.ren/2019-04-16-11-01-33.png" alt="2019-04-16-11-01-33"></li>
</ul>
<p>星型模型</p>
<p>星型模型中有两个重要的概念：事实表和维度表。<br>事实表：一些主键ID的集合，没有存放任何实际的内容<br>维度表：存放详细的数据信息，有唯一的主键ID。如上面的关键词表、用户表等等。<br><img src="http://img.wqkenqing.ren/2019-04-16-11-04-52.png" alt="2019-04-16-11-04-52"></p>
<p> 数据中心:<br>概念相对更大一些,可能即作为具体平台产品集合,也可能是一个团队行政划分.总得来说,是如</p>
<ul>
<li>大数据基础平台</li>
<li>数据仓库</li>
<li>DMP平台</li>
<li>相关应用平台如推荐系统,报表系统,可视化平台等.<br>产品集合组中.</li>
</ul>
<p>数据中台:<br>这个是由阿里于15年率先提出.主导思想是大中台,小前台.这块暂无特别明确的解释说法,但现在也有不少公司效仿.我个人从它的主导思想”大中台,小前台”的理解是,这个可能是体量更大,壁垒更少的一个数据集成体.比如阿里系的旗下公司,数据流都会归集到中台,同时阿里系下的公司也能获得不仅自己公司数据中心归集的数据反馈,还能获得阿里中台整合后流出的反馈数据.</p>
<h2 id="应用落地"><a href="#应用落地" class="headerlink" title="应用落地"></a>应用落地</h2><h3 id="公共服务"><a href="#公共服务" class="headerlink" title="公共服务"></a>公共服务</h3><ul>
<li>交通出行<br><img src="http://img.wqkenqing.ren/2019-04-16-16-15-42.png" alt="2019-04-16-16-15-42"></li>
<li>智慧城市</li>
<li>…</li>
</ul>
<h3 id="产品应用"><a href="#产品应用" class="headerlink" title="产品应用"></a>产品应用</h3><ul>
<li>用户画像</li>
<li>推荐系统</li>
<li>征信模型</li>
<li>精确营销</li>
<li>前沿科学(无人驾驶,人工智能,AR等)</li>
</ul>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>以上,就是我今天分享的主要内容.今天的主题是”器”,但对这些工具的讲解浅尝辄止,在实际的开发实战中涉及的情况是更为复杂,需要掌握的内容更多,深度也更深.我这里主要是想抛砖引玉,为大家提供一点自己的理解,若能有所帮助,不胜荣幸.</p>
<p>另外,工具始终是工具,菜刀再利也要厨子好,才能做好菜.所以如何利用这些工具,与我们的业务结合,实现我们想要的价值,这是我一直在探索的,也愿与各位同仁一同前行.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.wqkenqing.ren/2019/04/11/日常总结/kafka&spark/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kuiq  Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wong's bolg">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/11/日常总结/kafka&spark/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-11T14:44:17+08:00">
                2019-04-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="kafka-to-spark"><a href="#kafka-to-spark" class="headerlink" title="kafka to spark"></a>kafka to spark</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.wqkenqing.ren/2019/04/11/日常总结/spark学习2/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kuiq  Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wong's bolg">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/11/日常总结/spark学习2/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-11T09:36:54+08:00">
                2019-04-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="spark学习2"><a href="#spark学习2" class="headerlink" title="spark学习2"></a>spark学习2</h1><h2 id="spark-运行的四种模式"><a href="#spark-运行的四种模式" class="headerlink" title="spark 运行的四种模式"></a>spark 运行的四种模式</h2><h3 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h3><p>如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[1] ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100</span><br></pre></td></tr></table></figure></p>
<h3 id="standlone模式"><a href="#standlone模式" class="headerlink" title="standlone模式"></a>standlone模式</h3><h4 id="client"><a href="#client" class="headerlink" title="client"></a>client</h4><p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master spark://spark001:7077 –executor-memory 1G –total-executor-cores 1 ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100</p>
<h4 id="cluster"><a href="#cluster" class="headerlink" title="cluster"></a>cluster</h4><p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master spark://spark001:7077 –deploy-mode cluster –supervise –executor-memory 1G –total-executor-cores 1 ./lib/spark-examples-1.3.1-hadoop2.7.0.jar 100</p>
<h3 id="Yarn模式"><a href="#Yarn模式" class="headerlink" title="Yarn模式"></a>Yarn模式</h3><h4 id="client模式"><a href="#client模式" class="headerlink" title="client模式"></a>client模式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">client模式：</span><br><span class="line">结果xshell可见：</span><br><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --executor-memory 1G --num-executors 1 ./lib/spark-examples-1.3.1-hadoop2.7.0.jar 100</span><br></pre></td></tr></table></figure>
<h4 id="cluster模式"><a href="#cluster模式" class="headerlink" title="cluster模式"></a>cluster模式</h4><p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master yarn-cluster –executor-memory 1G –num-executors 1 ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100</p>
<h2 id="spark-sql"><a href="#spark-sql" class="headerlink" title="spark sql"></a>spark sql</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.wqkenqing.ren/2019/04/10/日常总结/flume记录/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kuiq  Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wong's bolg">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/10/日常总结/flume记录/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-10T13:58:05+08:00">
                2019-04-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="flume记录"><a href="#flume记录" class="headerlink" title="flume记录"></a>flume记录</h1><h2 id="from-kafka"><a href="#from-kafka" class="headerlink" title="from kafka"></a>from kafka</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = source1</span><br><span class="line"></span><br><span class="line">a1.sources.source1.type = org.apache.flume.source.kafka.KafkaSource</span><br><span class="line"></span><br><span class="line">a1.sources.source1.channels = c1</span><br><span class="line"></span><br><span class="line">a1.sources.source1.batchSize = 5000</span><br><span class="line"></span><br><span class="line">a1.sources.source1.batchDurationMillis = 2000</span><br><span class="line">a1.sources.source1.zookeeperConnect = localhost:2181</span><br><span class="line"></span><br><span class="line">#a1.sources.source1.kafka.brokerList = localhost:9092</span><br><span class="line">a1.sources.source1.kafka.bootstrap.servers = localhost:9092</span><br><span class="line">a1.sources.source1.topic = flumetest</span><br><span class="line">a1.sources.source1.kafka.consumer.group.id = custom.g.id</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line">a1.channels.c1.capacity = 10000</span><br><span class="line"></span><br><span class="line">a1.channels.c1.transactionCapacity = 10000</span><br><span class="line"></span><br><span class="line">a1.channels.c1.byteCapacityBufferPercentage = 20</span><br><span class="line"></span><br><span class="line">a1.channels.c1.byteCapacity = 800000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = file_roll</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.sink.directory = /home/hadoop/testfile/flume</span><br></pre></td></tr></table></figure>
<p>这里也有版本匹配的问题.经过多番尝试,这里的组合版本是flume1.6+kafka_2.11-2.2.0.tgz<br>其它版本可能会有request header 问题.<br>另外还遇到了指定topic 和 zookeeper的问题.</p>
<p>执行语句:flume-ng agent -n a1 -c conf -f kafka.properties -Dflume.root.logger=INFO,console</p>
<h2 id="flume-采集到kafka"><a href="#flume-采集到kafka" class="headerlink" title="flume 采集到kafka"></a>flume 采集到kafka</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">agent.sources=r1</span><br><span class="line">agent.sinks=k1</span><br><span class="line">agent.channels=c1</span><br><span class="line"></span><br><span class="line">agent.sources.r1.type=exec</span><br><span class="line">agent.sources.r1.command=tail /root/tomcat/logs/catalina.out</span><br><span class="line">agent.sources.r1.restart=true</span><br><span class="line">agent.sources.r1.batchSize=1000</span><br><span class="line">agent.sources.r1.batchTimeout=3000</span><br><span class="line">agent.sources.r1.channels=c1</span><br><span class="line"></span><br><span class="line">agent.channels.c1.type=memory</span><br><span class="line">agent.channels.c1.capacity=102400</span><br><span class="line">agent.channels.c1.transactionCapacity=1000</span><br><span class="line"></span><br><span class="line">agent.channels.c1.byteCapacity=134217728</span><br><span class="line">agent.channels.c1.byteCapacityBufferPercentage=80</span><br><span class="line"></span><br><span class="line">agent.sinks.k1.channel=c1</span><br><span class="line">agent.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">agent.sinks.k1.kafka.topic=sparkstreaming</span><br><span class="line">agent.sinks.k1.kafka.zookeeperConnect=47.102.199.215:2181</span><br><span class="line">#agent.sinks.k1.kafka.bootstrap.servers=47.102.199.215:9092</span><br><span class="line">agent.sinks.k1.kafka.brokerList =47.102.199.215:9092</span><br><span class="line">agent.sinks.k1.serializer.class=kafka.serializer.StringEncoder</span><br><span class="line">agent.sinks.k1.flumeBatchSize=1000</span><br><span class="line">agent.sinks.k1.useFlumeEventFormat=true</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.wqkenqing.ren/2019/04/10/日常总结/spark集群操作记录/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kuiq  Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wong's bolg">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/10/日常总结/spark集群操作记录/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-10T10:08:24+08:00">
                2019-04-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>#操作记录<br>  ./spark-submit   –master yarn-cluster  –class practice.spark.task.SparkTestDemo bigdata-jar-with-dependencies.jar  10</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.wqkenqing.ren/2019/04/10/日常总结/Yarn配置/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kuiq  Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wong's bolg">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/10/日常总结/Yarn配置/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-10T09:45:35+08:00">
                2019-04-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Yarn配置细节"><a href="#Yarn配置细节" class="headerlink" title="Yarn配置细节"></a>Yarn配置细节</h1><p>##内存,核数设置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">      &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;3072&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!--该配置用于配置任务请求时的资源. --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;2048&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;-Xmx3276m&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.wqkenqing.ren/2019/03/21/spark实战/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kuiq  Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wong's bolg">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/21/spark实战/" itemprop="url">spark实战</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-21T09:59:44+08:00">
                2019-03-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<h1 id="spark实战"><a href="#spark实战" class="headerlink" title="spark实战"></a>spark实战</h1><h2 id="spark-core"><a href="#spark-core" class="headerlink" title="spark-core"></a>spark-core</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.wqkenqing.ren/2019/03/19/日常总结/mapreduce组件总结/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kuiq  Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wong's bolg">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/19/日常总结/mapreduce组件总结/" itemprop="url">mapreduce组件总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-19T14:59:23+08:00">
                2019-03-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<h1 id="mapreduce组件总结"><a href="#mapreduce组件总结" class="headerlink" title="mapreduce组件总结"></a>mapreduce组件总结</h1><p>相关组件大致有</p>
<ol>
<li>Inputformat</li>
<li>Inputsplit</li>
<li>ReadRecorder</li>
<li>mapper</li>
<li>Combiner</li>
<li>Partioner</li>
<li>Reduce</li>
<li>GroupComparator</li>
<li>Reduce</li>
</ol>
<h1 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a>shuffle</h1><p><img src="http://img.wqkenqing.ren/2019-03-19-15-39-59.png" alt="2019-03-19-15-39-59"><br><img src="http://img.wqkenqing.ren/2019-03-19-16-46-06.png" alt="2019-03-19-16-46-06"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shuffle 被称为mapreduce的核心,一个真正让奇迹发生的地方.但它到底是什么呢?简练的讲,它就是 map out 到 reduce in 这段过程中对数据的处理过程.</span><br></pre></td></tr></table></figure>
<p>shuffle过程中主要发生的操作有,Partion,Sort,spill,merge,copy,sort,merge.(还有可能有combine操作)</p>
<p>具体流程是<br>map out后,Collector 对out后的数据进行处理. 数据将会写入到内存缓冲区,该内存缓冲区的数据达到80%后,会开启一个溢写线程,在磁盘本地创建一个文件.如果reduce设置了多个分区,写入buffer区的数据,会被打上一个分区标记.通过sortAndSpill()方法进行指对数据按分区号,key排序.最后溢出的文件是分区的,按key有序的文件.若buffer区中的20%一直未被填满,buffer写入进程不会断.但若达到100%,Buffer写入进程则会阻塞.并在buffer区中的数据全部spill完后才会再开启. (buffer区的内存默认是100M),spill过程中,若设置过combiner.则会对数据先进行combiner逻辑处理,再将处理后的数据写出</p>
<p>spill完成后则会对本地的spill后的文件进行Merge.即把多个spill后的文件进行合并,并排序.最后会行成一个有序文件</p>
<p>当1个Map Task 完成后,reduce 就会开启copy进程(默认是5个线程).这个过程中会通过http请求去各taskTracker(nodemanager),拉取相应的spill&amp;merge后的文件.<br>当copy完成后,则又会对数据进行merge.这个过程中同样有个类似map shuffle 中的buffer 溢写的阶段. 这个过程同样会触发combiner组件.这里的merge数据源有三种</p>
<ol>
<li>memory to memory</li>
<li>memory to disk</li>
<li>disk   to disk<br>默认1是不开启的.</li>
</ol>
<p>copy phase 完成后,是reduceTask 中的 sort phase<br>即对merge 中的文件继续进行sort and group .</p>
<p>当sort phase 完成.则开启reduce phase .到此shuffle正式完成.</p>
<p>##二次排序</p>
<pre><code>
</code></pre><p>mapreduce 常见的辅助排序</p>
<ol>
<li>partitioner</li>
<li>key的比较Comparator</li>
<li>分组函数Grouping Comparator</li>
</ol>
<h2 id="join"><a href="#join" class="headerlink" title="join"></a>join</h2><p>map join ,semi join ,reduce join</p>
<p>## </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.wqkenqing.ren/2019/03/14/日常总结/准备小结/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kuiq  Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wong's bolg">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/14/日常总结/准备小结/" itemprop="url">准备小结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-14T15:27:08+08:00">
                2019-03-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="准备小结"><a href="#准备小结" class="headerlink" title="准备小结"></a>准备小结</h1><h2 id="hdfs存储机制是怎样的"><a href="#hdfs存储机制是怎样的" class="headerlink" title="hdfs存储机制是怎样的?"></a>hdfs存储机制是怎样的?</h2><p>client端发送写文件请求，namenode检查文件是否存在，如果已存在，直接返回错误信息，否则，发送给client一些可用namenode节点<br>client将文件分块，并行存储到不同节点上datanode上，发送完成后，client同时发送信息给namenode和datanode<br>namenode收到的client信息后，发送确信信息给datanode<br>datanode同时收到namenode和datanode的确认信息后，提交写操作。</p>
<h2 id="hadoop中combiner的作用是什么"><a href="#hadoop中combiner的作用是什么" class="headerlink" title="hadoop中combiner的作用是什么?"></a>hadoop中combiner的作用是什么?</h2><p>当map生成的数据过大时，带宽就成了瓶颈，怎样精简压缩传给Reduce的数据，又不影响最终的结果呢。有一种方法就是使用Combiner，Combiner号称本地的Reduce，Reduce最终的输入，是Combiner的输出。</p>
<h2 id="你们数据库怎么导入hive-的-有没有出现问题"><a href="#你们数据库怎么导入hive-的-有没有出现问题" class="headerlink" title="你们数据库怎么导入hive 的,有没有出现问题"></a>你们数据库怎么导入hive 的,有没有出现问题</h2><p>在导入hive的时候，如果数据库中有blob或者text字段，会报错，解决方案在sqoop笔记中。在将数据由Oracle数据库导入到Hive时，发现带有clob字段的表的数据会错乱，出现一些字段全为NULL的空行。由于在项目中CLOB字段没有实际的分析用途，因此考虑将CLOB字段去掉。</p>
<h2 id="hdfs-site-xml的3个主要属性"><a href="#hdfs-site-xml的3个主要属性" class="headerlink" title="hdfs-site.xml的3个主要属性?"></a>hdfs-site.xml的3个主要属性?</h2><p>dfs.name.dir决定的是元数据存储的路径以及DFS的存储方式(磁盘或是远端)<br>dfs.data.dir决定的是数据存储的路径<br>fs.checkpoint.dir用于第二Namenode</p>
<h2 id="下列哪项通常是集群的最主要瓶颈"><a href="#下列哪项通常是集群的最主要瓶颈" class="headerlink" title="下列哪项通常是集群的最主要瓶颈"></a>下列哪项通常是集群的最主要瓶颈</h2><p>磁盘 IO<br>答案：C 磁盘<br>首先集群的目的是为了节省成本，用廉价的 pc 机，取代小型机及大型机。小型机和大型机有什么特点？<br>1.cpu 处理能力强<br>2.内存够大，所以集群的瓶颈不可能是 a 和 d<br>3.如果是互联网有瓶颈，可以让集群搭建内网。每次写入数据都要通过网络（集群是内网），然后还要写入 3 份数据，所以 IO 就会打折扣。</p>
<h2 id="关于-SecondaryNameNode-哪项是正确的？"><a href="#关于-SecondaryNameNode-哪项是正确的？" class="headerlink" title="关于 SecondaryNameNode 哪项是正确的？"></a>关于 SecondaryNameNode 哪项是正确的？</h2><p>它的目的是帮助 NameNode 合并编辑日志，减少 NameNode 启动时间 </p>
<h2 id="mapreduce的原理"><a href="#mapreduce的原理" class="headerlink" title="mapreduce的原理?"></a>mapreduce的原理?</h2><p>MapReduce采用”分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个分节点共同完成，然后通过整合各个节点的中间结果，<br>得到最终结果。简单地说，MapReduce就是”任务的分解与结果的汇总”。<br>在Hadoop中，用于执行MapReduce任务的机器角色有两个：一个是JobTracker；另一个是TaskTracker，JobTracker是用于调度工作的，TaskTracker<br>是用于执行工作的。一个Hadoop集群中只有一台JobTracker。<br>在分布式计算中，MapReduce框架负责处理了并行编程中分布式存储、工作调度、负载均衡、容错均衡、容错处理以及网络通信等复杂问题，把处理<br>过程高度抽象为两个函数：map和reduce，map负责把任务分解成多个任务，reduce负责把分解后多任务处理的结果汇总起来。<br>需要注意的是，用MapReduce来处理的数据集（或任务）必须具备这样的特点：待处理的数据集可以分解成许多小的数据集，而且每一个小数据集都<br>可以完全并行地进行处理。</p>
<h2 id="HDFS存储的机制"><a href="#HDFS存储的机制" class="headerlink" title="HDFS存储的机制?"></a>HDFS存储的机制?</h2><h3 id="写流程："><a href="#写流程：" class="headerlink" title="写流程："></a>写流程：</h3><p>client链接namenode存数据<br>namenode记录一条数据位置信息（元数据），告诉client存哪。<br>client用hdfs的api将数据块（默认是64M）存储到datanode上。<br>datanode将数据水平备份。并且备份完将反馈client。<br>client通知namenode存储块完毕。<br>namenode将元数据同步到内存中。<br>另一块循环上面的过程。</p>
<h3 id="读流程"><a href="#读流程" class="headerlink" title="读流程"></a>读流程</h3><h2 id="举一个简单的例子说明mapreduce是怎么来运行的"><a href="#举一个简单的例子说明mapreduce是怎么来运行的" class="headerlink" title="举一个简单的例子说明mapreduce是怎么来运行的 ?"></a>举一个简单的例子说明mapreduce是怎么来运行的 ?</h2><p>MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。<br>　　Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。<br>Mapper任务的执行过程详解<br>　　每个Mapper任务是一个Java进程，它会读取HDFS中的文件，解析成很多的键值对，经过我们覆盖的map方法处理后，<br>转换为很多的键值对再输出。整个Mapper任务的处理过程又可以分为以下六个阶段：<br>　　第一阶段是把输入文件按照一定的标准分片(InputSplit)，每个输入片的大小是固定的。默认情况下，输入片(InputSplit)<br>的大小与数据块(Block)的大小是相同的。如果数据块(Block)的大小是默认值128MB，输入文件有两个，一个是32MB，一个是　172MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由　一个Mapper进程处理。这里的三个输入片，会有三个Mapper进程处理。</p>
<p>　　第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键”是每一　行的起始位置(单位是字节)，“值”是本行的文本内容。<br>　　<br>　　第三阶段是调用Mapper类中的map方法。第二阶段中解析出来的每一个键值对，调用一次map方法。如果有1000个键值对，就会　调用1000次map方法。每一次调用map方法会输出零个或者多个键值对。</p>
<p>　　第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份(如北京、上海、　山东等)，那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是Reducer　任务运行的数量。默认只有一个Reducer任务。<br>第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值　对&lt;2,2&gt;、&lt;1,3&gt;、&lt;2,1&gt;，键和值分别是整数。那么排序后的结果是&lt;1,3&gt;、&lt;2,1&gt;、&lt;2,2&gt;。如果有第六阶段，那么进入</p>
<p>第六阶段　如果没有，直接输出到本地的Linux文件中。　第六阶段是对数据进行归约处理，也就是reduce处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。　归约后的数据输出到本地的linxu文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。　Reducer任务的执行过程详解<br>每个Reducer任务是一个java进程。Reducer任务接收Mapper任务的输出，归约处理后写入到HDFS中，可以分为三个阶段：<br>第一阶段是Reducer任务会主动从Mapper任务复制其输出的键值对。Mapper任务可能会有很多，因此Reducer会复制多个Mapper的输出。<br>第二阶段是把复制到Reducer本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。<br>第三阶段是对排序后的键值对调用reduce方法。键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对。<br>最后把这些输出的键值对写入到HDFS文件中。<br>在整个MapReduce程序的开发过程中，我们最大的工作量是覆盖map函数和覆盖reduce函数。</p>
<h2 id="了解hashMap-和hashTable吗介绍下，他们有什么区别。"><a href="#了解hashMap-和hashTable吗介绍下，他们有什么区别。" class="headerlink" title="了解hashMap 和hashTable吗介绍下，他们有什么区别。"></a>了解hashMap 和hashTable吗介绍下，他们有什么区别。</h2><h2 id="为什么重写equals还要重写hashcode"><a href="#为什么重写equals还要重写hashcode" class="headerlink" title="为什么重写equals还要重写hashcode"></a>为什么重写equals还要重写hashcode</h2><p>因为equals比较的是内容是一致.但hashcode</p>
<h2 id="说一下map的分类和常见的情况"><a href="#说一下map的分类和常见的情况" class="headerlink" title="说一下map的分类和常见的情况"></a>说一下map的分类和常见的情况</h2><p> hashmap,hashtable,treemap,LinkedHashMap</p>
<ul>
<li>根据键得到值，因此不允许键重复(重复了覆盖了),但允许值重复<h3 id="Hashmap"><a href="#Hashmap" class="headerlink" title="Hashmap"></a>Hashmap</h3>是一个最常用的Map</li>
<li>它根据键的HashCode值存储数据,根据键可以直接获取它的值，具有很快的访问速度，遍历时，取得数据的顺序是完全随机的</li>
<li>最多只允许一条记录的键为Null;允许多条记录的值为 Null;</li>
<li>HashMap不支持线程的同步，即任一时刻可以有多个线程同时写HashMap;可能会导致数据的不一致。</li>
<li>如果需要同步，可以用 Collections的synchronizedMap方法使HashMap具有同步的能力，或者使用ConcurrentHashMap<h3 id="Hashtable"><a href="#Hashtable" class="headerlink" title="Hashtable"></a>Hashtable</h3>Hashtable与 HashMap类似,它继承自Dictionary类,不同的是:它不允许记录的键或者值为空;</li>
<li>它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了 Hashtable在写入时会比较慢<h3 id="LinkedHashMap"><a href="#LinkedHashMap" class="headerlink" title="LinkedHashMap"></a>LinkedHashMap</h3>是 HashMap 的一个子类，保存了记录的插入顺序，在用 Iterator 遍历 LinkedHashMap 时，先得到的记录肯定是先插入的.<br>也可以在构造时用带参数，按照应用次数排序。在遍历的时候会比 HashMap 慢，不过有种情况例外，当 HashMap 容量很大，实际数据较少时，遍历起来可能会比 LinkedHashMap 慢，因为 LinkedHashMap 的遍历速度只和实际数据有关，和容量无关，而 HashMap 的遍历速度和他的容量有关<h3 id="TreeMap"><a href="#TreeMap" class="headerlink" title="TreeMap"></a>TreeMap</h3>实现 SortMap 接口,能够把它保存的记录根据键排序, 默认是按键值的升序排序，也可以指定排序的比较器，当用 Iterator 遍历 TreeMap 时，得到的记录是排过序的</li>
</ul>
<p>HashMap，链表法存储，entry[]数组，线程不安全，可能死锁 concurrentHashMap，segment数组，每个segent下维护一组entry[]数组，每个segment是一把锁，线程安全 LinkedHashMap</p>
<hr>
<h2 id="Object若不重写hashCode-的话，hashCode-如何计算出来的？"><a href="#Object若不重写hashCode-的话，hashCode-如何计算出来的？" class="headerlink" title="Object若不重写hashCode()的话，hashCode()如何计算出来的？"></a>Object若不重写hashCode()的话，hashCode()如何计算出来的？</h2><p>hashcode采用的是</p>
<h2 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h2><h3 id="1-spark的有几种部署模式，每种模式特点？"><a href="#1-spark的有几种部署模式，每种模式特点？" class="headerlink" title="1. spark的有几种部署模式，每种模式特点？"></a>1. spark的有几种部署模式，每种模式特点？</h3><h4 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h4><p>本地模式分三类</p>
<ul>
<li>local：只启动一个executor</li>
<li>local[k]: 启动k个executor</li>
<li>local[*]：启动跟cpu数目相同的 executor</li>
</ul>
<h3 id="cluster模式"><a href="#cluster模式" class="headerlink" title="cluster模式"></a>cluster模式</h3><p>cluster模式肯定就是运行很多机器上了，但是它又分为以下三种模式，区别在于谁去管理资源调度。（说白了，就好像后勤管家，哪里需要资源，后勤管家要负责调度这些资源）</p>
<h4 id="standalone模式"><a href="#standalone模式" class="headerlink" title="standalone模式"></a>standalone模式</h4><p>分布式部署集群，自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础</p>
<h4 id="Spark-on-yarn模式"><a href="#Spark-on-yarn模式" class="headerlink" title="Spark on yarn模式"></a>Spark on yarn模式</h4><p>分布式部署集群，资源和任务监控交给yarn管理<br>粗粒度资源分配方式，包含cluster和client运行模式<br>cluster 适合生产，driver运行在集群子节点，具有容错功能<br>client 适合调试，dirver运行在客户端</p>
<h3 id="2-Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？"><a href="#2-Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？" class="headerlink" title="2. Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？"></a>2. Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？</h3><h4 id="Spark-core"><a href="#Spark-core" class="headerlink" title="Spark core"></a>Spark core</h4><p>是其它组件的基础，spark的内核<br>主要包含：有向循环图、RDD、Lingage、Cache、broadcast等</p>
<h4 id="SparkStreaming"><a href="#SparkStreaming" class="headerlink" title="SparkStreaming"></a>SparkStreaming</h4><p>是一个对实时数据流进行高通量、容错处理的流式处理系统<br>将流式计算分解成一系列短小的批处理作业</p>
<h4 id="Spark-sql："><a href="#Spark-sql：" class="headerlink" title="Spark sql："></a>Spark sql：</h4><p>能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询</p>
<h4 id="MLBase"><a href="#MLBase" class="headerlink" title="MLBase"></a>MLBase</h4><p>是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低<br>MLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime。</p>
<h4 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h4><p>是Spark中用于图和图并行计算</p>
<h4 id="spark有哪些组件"><a href="#spark有哪些组件" class="headerlink" title="spark有哪些组件"></a>spark有哪些组件</h4><p>master：管理集群和节点，不参与计算。<br>worker：计算节点，进程本身不参与计算，和master汇报。<br>Driver：运行程序的main方法，创建spark context对象。<br>spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。<br>client：用户提交程序的入口。</p>
<ul>
<li><a href="https://blog.csdn.net/yirenboy/article/details/47441465" target="_blank" rel="noopener">https://blog.csdn.net/yirenboy/article/details/47441465</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.wqkenqing.ren/2019/03/11/日常总结/kafka/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kuiq  Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wong's bolg">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/11/日常总结/kafka/" itemprop="url">kafka</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-11T09:33:43+08:00">
                2019-03-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic sparkstreaming<br>kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic flumetest</p>
<p>kafka-console-producer.sh –broker-list localhost:9092 –topic flumetest :创建生产者</p>
<p>kafka-console-consumer.sh –bootstrap-server namenode:9092  –topic sparkstreaming</p>
<h1 id="Kafka相关小结"><a href="#Kafka相关小结" class="headerlink" title="Kafka相关小结"></a>Kafka相关小结</h1><h2 id="kafka-相关指令"><a href="#kafka-相关指令" class="headerlink" title="kafka 相关指令"></a>kafka 相关指令</h2><p>kafka-server-start.sh config/server.properties &amp; 启动<br>kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic topic_name  :创建topic<br>kafka-console-producer.sh –broker-list localhost:9092 –topic topic_name :创建生产者</p>
<p>kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic topic_name :创建消费者</p>
<p>kafka-console-producer.sh –broker-list namenode:9092 –topic sparkstreaming</p>
<h2 id="kafka-java-api"><a href="#kafka-java-api" class="headerlink" title="kafka java api"></a>kafka java api</h2><p>kafka 虽然搭建较为简单,但想要对针它编程体验还是有些问题.初步使用下来明显感觉对版本的强约束性.以我线上版本<br><img src="http://img.wqkenqing.ren/2019-03-12-11-02-43.png" alt="2019-03-12-11-02-43">为例,我java项目对应的版本则是<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;kafka_2.10&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;0.8.1&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;0.8.2.1&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>
<p>以上版本搭配经由我亲测通过</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Kuiq  Wang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kuiq  Wang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
